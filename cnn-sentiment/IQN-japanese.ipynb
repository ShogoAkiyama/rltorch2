{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext import data, datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, date in enumerate(range(2011, 2019)):\n",
    "    tmp = pd.read_csv('./data/news/' + str(date) + '.csv', encoding='cp932')\n",
    "    tmp = tmp[tmp['Company_IDs(TSE)'] == '7203']\n",
    "    tmp = tmp[['Time_Stamp_Original(JST)', \n",
    "                        'Company_Code(TSE)', \n",
    "                        'Headline', \n",
    "                        'News_Source',\n",
    "                        'Company_Relevance', \n",
    "                        'Keyword_Article']]\n",
    "\n",
    "    # 欠損除去\n",
    "    tmp = tmp[~tmp[\"Keyword_Article\"].isnull()]\n",
    "\n",
    "    # タグ除去\n",
    "    tmp = tmp[(tmp['News_Source'] == '日経') | \n",
    "                        (tmp['News_Source'] == 'ＮＱＮ') |\n",
    "                        (tmp['News_Source'] == 'ＱＵＩＣＫ') | \n",
    "                        (tmp['News_Source'] == 'Ｒ＆Ｉ')]\n",
    "\n",
    "    tmp.index = pd.to_datetime(tmp[\"Time_Stamp_Original(JST)\"])\n",
    "    tmp = tmp.drop(\"Time_Stamp_Original(JST)\", axis=1)\n",
    "    \n",
    "    if i == 0:\n",
    "        df1 = tmp.copy()\n",
    "    else:\n",
    "        df1 = pd.concat([df1, tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インデックスを設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_time(x):\n",
    "    if x.hour > 15:\n",
    "        return x + datetime.timedelta(days=1)\n",
    "    return x\n",
    "\n",
    "time = pd.to_datetime(df1.index.values)\n",
    "df1.index = df1.index.map(norm_time)\n",
    "df1.index = df1.index.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 株価を挿入する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>3265.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>3295.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>3380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>3455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-11</th>\n",
       "      <td>3455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-12</th>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-13</th>\n",
       "      <td>3535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-14</th>\n",
       "      <td>3550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-17</th>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-18</th>\n",
       "      <td>3510.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            adj_close\n",
       "2011-01-04     3265.0\n",
       "2011-01-05     3295.0\n",
       "2011-01-06     3380.0\n",
       "2011-01-07     3455.0\n",
       "2011-01-11     3455.0\n",
       "2011-01-12     3500.0\n",
       "2011-01-13     3535.0\n",
       "2011-01-14     3550.0\n",
       "2011-01-17     3500.0\n",
       "2011-01-18     3510.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 株価を取り出す\n",
    "df2 = pd.read_csv('./data/stock_price/7203.csv', index_col=0)\n",
    "df2.index = pd.to_datetime(df2['date'])\n",
    "df2.index = df2.index.date\n",
    "df2 = df2.drop(['date'], axis=1)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 時系列をくっつける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ts-zemi/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.concat([df1,df2], axis=1, join_axes=[df1.index])\n",
    "df3['price'] = np.round(df2.pct_change().shift(-1) * 100, 10)\n",
    "df3['Keyword_Article'] = \\\n",
    "    df3.groupby(level=0).apply(lambda x: ':<pad>:'.join(list(x['Keyword_Article'])))\n",
    "df3 = df3.dropna()\n",
    "\n",
    "df3 = df3[~df3.duplicated(subset=['Keyword_Article'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Code(TSE)</th>\n",
       "      <th>Headline</th>\n",
       "      <th>News_Source</th>\n",
       "      <th>Company_Relevance</th>\n",
       "      <th>Keyword_Article</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;日経&gt;◇次世代車の研究開発　名大に国内最大拠点</td>\n",
       "      <td>日経</td>\n",
       "      <td>38</td>\n",
       "      <td>安全:環境:負荷:開発:目指す:開所式:研究拠点:効率:簡素化:次世代:電気自動車:電気:幅...</td>\n",
       "      <td>3265.0</td>\n",
       "      <td>0.918836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;日経&gt;◇12月の中国新車販売、トヨタが単月で過去最高</td>\n",
       "      <td>日経</td>\n",
       "      <td>100</td>\n",
       "      <td>北京:中国:１２月:新車販売台数:前年同月比:増:過去最高:制限:受け:全国:各地:乗用車:...</td>\n",
       "      <td>3295.0</td>\n",
       "      <td>2.579666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;NQN&gt;◇トヨタ社長「今年は後半に晴れ間」　為替は１ドル＝90円を期待</td>\n",
       "      <td>ＮＱＮ</td>\n",
       "      <td>100</td>\n",
       "      <td>豊田:見通し:販売:エコカー補助金:安定的:伸び:株価:為替:水準:日経平均株価:最低:ライ...</td>\n",
       "      <td>3380.0</td>\n",
       "      <td>2.218935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;日経&gt;◇福岡県、自動車の技術者育成へ新組織　年内、中小向け</td>\n",
       "      <td>日経</td>\n",
       "      <td>37</td>\n",
       "      <td>自動車産業:強化:福岡:先端:設置:方針:技術:調査:ニーズ:カリキュラム:大学:受け:生産...</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-11</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;日経&gt;◇トヨタ、米ミシガン州に安全研究センター新設</td>\n",
       "      <td>日経</td>\n",
       "      <td>100</td>\n",
       "      <td>先進:安全:子供:高齢者:事故:向上:目指す:米国:大規模:リコール:回収:問題:開催:豊田...</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>1.302460</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Company_Code(TSE)                              Headline  \\\n",
       "2011-01-04             7203.0              <日経>◇次世代車の研究開発　名大に国内最大拠点   \n",
       "2011-01-05             7203.0           <日経>◇12月の中国新車販売、トヨタが単月で過去最高   \n",
       "2011-01-06             7203.0  <NQN>◇トヨタ社長「今年は後半に晴れ間」　為替は１ドル＝90円を期待   \n",
       "2011-01-07             7203.0        <日経>◇福岡県、自動車の技術者育成へ新組織　年内、中小向け   \n",
       "2011-01-11             7203.0            <日経>◇トヨタ、米ミシガン州に安全研究センター新設   \n",
       "\n",
       "           News_Source Company_Relevance  \\\n",
       "2011-01-04          日経                38   \n",
       "2011-01-05          日経               100   \n",
       "2011-01-06         ＮＱＮ               100   \n",
       "2011-01-07          日経                37   \n",
       "2011-01-11          日経               100   \n",
       "\n",
       "                                              Keyword_Article  adj_close  \\\n",
       "2011-01-04  安全:環境:負荷:開発:目指す:開所式:研究拠点:効率:簡素化:次世代:電気自動車:電気:幅...     3265.0   \n",
       "2011-01-05  北京:中国:１２月:新車販売台数:前年同月比:増:過去最高:制限:受け:全国:各地:乗用車:...     3295.0   \n",
       "2011-01-06  豊田:見通し:販売:エコカー補助金:安定的:伸び:株価:為替:水準:日経平均株価:最低:ライ...     3380.0   \n",
       "2011-01-07  自動車産業:強化:福岡:先端:設置:方針:技術:調査:ニーズ:カリキュラム:大学:受け:生産...     3455.0   \n",
       "2011-01-11  先進:安全:子供:高齢者:事故:向上:目指す:米国:大規模:リコール:回収:問題:開催:豊田...     3455.0   \n",
       "\n",
       "               price  \n",
       "2011-01-04  0.918836  \n",
       "2011-01-05  2.579666  \n",
       "2011-01-06  2.218935  \n",
       "2011-01-07  0.000000  \n",
       "2011-01-11  1.302460  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csvファイルに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date = 2015\n",
    "test_date = 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.concat([df3[['Keyword_Article', 'price']].rename(\n",
    "                                      columns={'Keyword_Article': 'state', 'price': 'reward'}),\n",
    "                               df3[['Keyword_Article']].shift(-1).rename(\n",
    "                                      columns={'Keyword_Article': 'next_state'})], axis=1).dropna()\n",
    "df4 = df4[['state', 'next_state', 'reward']]\n",
    "\n",
    "date_year = df4.index.map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[date_year <= train_date].to_csv(\n",
    "        './data/news/text_train.tsv',\n",
    "        header=None,\n",
    "        index=None,\n",
    "        sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[(train_date < date_year) & (date_year < test_date)].to_csv(\n",
    "        './data/news/text_val.tsv',\n",
    "        header=None,\n",
    "        index=None,\n",
    "        sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[test_date <= date_year].to_csv(\n",
    "        './data/news/text_test.tsv',\n",
    "        header=None,\n",
    "        index=None,\n",
    "        sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing_text(text):\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\") or (p == \":\") or (p == \"<\")or (p == \">\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    # ピリオドなどの前後にはスペースを入れておく\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
    "def tokenizer_punctuation(text):\n",
    "    return text.strip().split(':')\n",
    "\n",
    "# 前処理と分かち書きをまとめた関数を定義\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer_punctuation(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# 読み込んだ内容に対して行う処理を定義\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, \n",
    "                            use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, \n",
    "                            init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/news', train='text_train.tsv',\n",
    "    format='tsv',\n",
    "    fields=[('Text1', TEXT), ('Text2', TEXT), ('Label', LABEL)])\n",
    "train_ds = train_ds[0]\n",
    "\n",
    "# japanese_fasttext_vectors = Vectors(name='./data/news/cc.ja.300.vec')\n",
    "TEXT.build_vocab(train_ds, \n",
    "#                  vectors=japanese_fasttext_vectors,\n",
    "                 min_freq=10)\n",
    "TEXT.vocab.freqs\n",
    "\n",
    "train_dl = torchtext.data.Iterator(\n",
    "    train_ds, batch_size=batch_size, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   2, 1341,  466,  ...,    1,    1,    1],\n",
      "        [   2,    4, 1016,  ...,    1,    1,    1],\n",
      "        [   2,  234,   37,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,  506,  259,  ...,    1,    1,    1],\n",
      "        [   2,    7,  255,  ...,    1,    1,    1],\n",
      "        [   2,  395,  714,  ...,    1,    1,    1]]), tensor([151, 160,  29, 109,  90,  13,  60,  52,  40,  13,   6, 148, 195,  15,\n",
      "         23, 155, 172, 142,  64, 198,  51, 106,   9,  16,  23,  54, 108,  38,\n",
      "        118,  77,  74, 113]))\n",
      "(tensor([[   2,   77,  486,  ...,    1,    1,    1],\n",
      "        [   2,    4,  233,  ...,    1,    1,    1],\n",
      "        [   2,    0,  105,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,  280,   24,  ...,    1,    1,    1],\n",
      "        [   2,    0, 1447,  ...,    1,    1,    1],\n",
      "        [   2,  794,  140,  ...,    1,    1,    1]]), tensor([125,  26,  25, 256,  43,  76,  17, 276,  43,  30, 172,  70, 328,  24,\n",
      "        111,  35,  51,  21,  70,  60,  86,  36,  22,  17,  94,  62, 105, 331,\n",
      "         70, 142,  75, 249]))\n",
      "tensor([ 1.0588, -1.6923,  1.7742, -1.5366, -2.0537, -0.0689,  0.3086,  0.0634,\n",
      "         0.7886,  0.5325,  1.7863,  0.1595, -1.0886,  3.7037, -0.3130,  2.5723,\n",
      "         1.2500,  1.2956,  2.2247,  0.1533, -0.6173, -0.1516, -0.9368, -2.1505,\n",
      "         0.2142, -0.7974, -2.6190,  0.0000, -2.6235,  2.0063,  1.3713,  0.5164])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "print(batch.Text1)\n",
    "print(batch.Text2)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1000])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.Text1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IQN(nn.Module):\n",
    "    def __init__(self, text_embedding_vector, vocab_size, embedding_dim,\n",
    "                 n_filters, filter_sizes, pad_idx,\n",
    "                 d_model=300, n_actions=1, n_quant=8):\n",
    "        super().__init__()\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.n_quant = n_quant\n",
    "        \n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size, \n",
    "            embedding_dim, \n",
    "            padding_idx=pad_idx)\n",
    "\n",
    "\n",
    "        self.phi = nn.Linear(64, 64)\n",
    "        # self.phi_bias = nn.Parameter(torch.zeros(64))\n",
    "\n",
    "        # self.fc0 = nn.Linear(n_state, 64)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=n_filters,\n",
    "                      kernel_size=(fs, embedding_dim))\n",
    "            for fs in filter_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc0 = nn.Linear(len(filter_sizes) * n_filters, 64)\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc_q = nn.Linear(64, n_actions)\n",
    "\n",
    "    def forward(self, text):\n",
    "        # [batch_size, num_state]\n",
    "        mb_size = text.size(0)\n",
    "\n",
    "        # [batch_size, sen_len] → [batch_size, sen_len, emb_dim]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        # [batch size, 1, sen len, emb dim]\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        # [batch size, n_filters, sen len - filter_sizes[n] + 1]\n",
    "        x = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "\n",
    "        # [batch size, n_filters, sen len - filter_sizes[n] + 1]\n",
    "        x = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in x]\n",
    "\n",
    "        # [batch size, n_filters * len(filter_sizes)]\n",
    "        x = torch.cat(x, dim=1)\n",
    "\n",
    "        # [batch size, 64]\n",
    "        x = F.relu(self.fc0(x))\n",
    "\n",
    "        # [batch_size, 64] \n",
    "        # → [batch_size, quant, 64]\n",
    "        x = x.repeat(1, 1, self.n_quant).view(mb_size, self.n_quant, 64)\n",
    "\n",
    "        # [batch_size, quant, 1]\n",
    "        tau = torch.rand(mb_size, self.n_quant).to(self.device).view(mb_size, -1, 1)\n",
    "\n",
    "        # [1, 64]\n",
    "        pi_mtx = math.pi * torch.arange(0, 64, 1.0).to(self.device).unsqueeze(0)\n",
    "\n",
    "        # [batch_size, quant, 64]\n",
    "        cos_tau = torch.cos(torch.matmul(tau, pi_mtx))\n",
    "\n",
    "        # [batch_size, quant, quantile_embedding_di64]\n",
    "        phi = F.relu(self.phi(cos_tau))\n",
    "\n",
    "        # [batch_size, quant, 4]\n",
    "        h = x * phi\n",
    "\n",
    "        # [batch_size, quant, 64]\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "\n",
    "        # [batch_size, quant, 2]\n",
    "        action_value = self.fc_q(h).view(mb_size, self.n_quant, self.n_actions)\n",
    "        # action_value = action_value.transpose(0, 2, 1)\n",
    "\n",
    "        return action_value, tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber(x):\n",
    "        cond = (x.abs() < 1.0).float().detach()\n",
    "        return 0.5 * x.pow(2) * cond + (x.abs() - 0.5) * (1.0 - cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab.freqs)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "PAD_IDX = 1\n",
    "N_QUANT = 8\n",
    "GAMMA = 0.99\n",
    "cumulative_density = torch.FloatTensor(\n",
    "            (2 * np.arange(NUM_QUANTILE) + 1) / (2.0 * NUM_QUANTILE)).to(device)\n",
    "\n",
    "quantile_weight = 1.0 / NUM_QUANTILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IQN(TEXT.vocab.vectors, VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS,\n",
    "                        FILTER_SIZES, PAD_IDX)\n",
    "\n",
    "target_model = IQN(TEXT.vocab.vectors, VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS,\n",
    "                        FILTER_SIZES, PAD_IDX)\n",
    "\n",
    "model = model.to(device)\n",
    "target_model = target_model.to(device)\n",
    "\n",
    "target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法\n",
    "learning_rate = 2.5e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(scores, y):    \n",
    "    correct = (scores == y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()\n",
    "    return acc\n",
    "\n",
    "def huber(x):\n",
    "        cond = (x.abs() < 1.0).float().detach()\n",
    "        return 0.5 * x.pow(2) * cond + (x.abs() - 0.5) * (1.0 - cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## テスト\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----start----\n",
      "--------------------\n",
      "epoch: 0\n",
      "loss: 3.9972290992736816\n",
      "epi_reward: 220.2044517993927\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 1\n",
      "loss: 1.9217147827148438\n",
      "epi_reward: 243.45103645324707\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 2\n",
      "loss: 1.132009744644165\n",
      "epi_reward: 242.27089858055115\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 3\n",
      "loss: 1.3228102922439575\n",
      "epi_reward: 216.55266785621643\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 4\n",
      "loss: 1.319065809249878\n",
      "epi_reward: 225.5452779531479\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 5\n",
      "loss: 1.288326382637024\n",
      "epi_reward: 219.20462918281555\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 6\n",
      "loss: 1.407668113708496\n",
      "epi_reward: 225.32276141643524\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 7\n",
      "loss: 1.3932201862335205\n",
      "epi_reward: 222.48839497566223\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 8\n",
      "loss: 1.405043125152588\n",
      "epi_reward: 208.50568902492523\n",
      "neutrals: 0   buys: 0\n",
      "--------------------\n",
      "epoch: 9\n",
      "loss: 1.6536020040512085\n",
      "epi_reward: 203.37207353115082\n",
      "neutrals: 0   buys: 0\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "# dataloaders_dict = {'train': train_dl, 'val':val_dl}\n",
    "dataloaders_dict = {'train': train_dl}\n",
    "\n",
    "print('----start----')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epi_rewards = []\n",
    "    neutrals = []\n",
    "    buys = []\n",
    "    \n",
    "    # update target_model\n",
    "    if epoch % TARGET_UPDATE_FREQ == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    for batch in (dataloaders_dict['train']):      \n",
    "        # curr_q\n",
    "        states = batch.Text1[0].to(device)\n",
    "        next_states = batch.Text2[0].to(device)\n",
    "        rewards = batch.Label.to(device)\n",
    "    \n",
    "        curr_q, tau = model(states)\n",
    "        action_value = curr_q.mean(dim=1)\n",
    "        action_value[action_value > 0] = 1\n",
    "        action_value[action_value <= 0] = 0\n",
    "        selected_actions = action_value.cpu().detach().numpy().reshape(-1)\n",
    "        \n",
    "        epi_rewards.append((\n",
    "            selected_actions * rewards.detach().cpu().numpy()).sum())\n",
    "        \n",
    "\n",
    "        # target_q\n",
    "        with torch.no_grad():\n",
    "            next_q, _ = target_model(next_states)\n",
    "            next_q = next_q.squeeze(2)\n",
    "\n",
    "            target_q = rewards.reshape(-1, 1) + GAMMA * next_q\n",
    "            target_q = target_q.unsqueeze(2)\n",
    "            target_q = target_q.repeat(1, 1, N_QUANT)\n",
    "            target_q = target_q.permute(0, 2, 1)\n",
    "\n",
    "        # (BATCH, N_QUANT, N_QUANT)\n",
    "        tau = tau.repeat(1, 1, N_QUANT)\n",
    "        diff = target_q - curr_q\n",
    "\n",
    "        loss = huber(diff)\n",
    "\n",
    "        I_delta = (diff<0).double()\n",
    "        loss *= torch.abs(tau - I_delta)\n",
    "\n",
    "        # huber loss\n",
    "        loss = torch.mean(torch.sum(torch.mean(loss, dim=2), dim=1))\n",
    "\n",
    "        # backprop loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('--------------------')\n",
    "    print('epoch:', epoch)\n",
    "    print('loss:', loss.item())\n",
    "    print('epi_reward:', sum(epi_rewards))\n",
    "    print('neutrals:', sum(neutrals), '  buys:', sum(buys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "states = batch.Text1[0].to(device)\n",
    "next_states = batch.Text2[0].to(device)\n",
    "rewards = batch.Label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_q.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = states[8]\n",
    "curr_q, tau = model(state.unsqueeze(0))\n",
    "dist_action = curr_q.view(-1).cpu().detach().numpy()\n",
    "sns.distplot(dist_action, bins=51, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
