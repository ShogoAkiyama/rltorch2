{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "import string\n",
    "import math\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from torchtext import data, datasets\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from model import QRDQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, date in enumerate(range(2011, 2019)):\n",
    "    tmp = pd.read_csv('./data/news/' + str(date) + '.csv', encoding='cp932')\n",
    "    tmp = tmp[tmp['Company_IDs(TSE)'] == '7203']\n",
    "    tmp = tmp[['Time_Stamp_Original(JST)', \n",
    "                        'Company_Code(TSE)', \n",
    "                        'Headline', \n",
    "                        'News_Source',\n",
    "                        'Company_Relevance', \n",
    "                        'Keyword_Article']]\n",
    "\n",
    "    # 欠損除去\n",
    "    tmp = tmp[~tmp[\"Keyword_Article\"].isnull()]\n",
    "\n",
    "    # タグ除去\n",
    "    tmp = tmp[(tmp['News_Source'] == '日経') | \n",
    "                        (tmp['News_Source'] == 'ＮＱＮ') |\n",
    "                        (tmp['News_Source'] == 'ＱＵＩＣＫ') | \n",
    "                        (tmp['News_Source'] == 'Ｒ＆Ｉ')]\n",
    "\n",
    "    tmp.index = pd.to_datetime(tmp[\"Time_Stamp_Original(JST)\"])\n",
    "    tmp = tmp.drop(\"Time_Stamp_Original(JST)\", axis=1)\n",
    "    \n",
    "    if i == 0:\n",
    "        df1 = tmp.copy()\n",
    "    else:\n",
    "        df1 = pd.concat([df1, tmp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# インデックスを設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_time(x):\n",
    "    if x.hour > 15:\n",
    "        return x + datetime.timedelta(days=1)\n",
    "    return x\n",
    "\n",
    "time = pd.to_datetime(df1.index.values)\n",
    "df1.index = df1.index.map(norm_time)\n",
    "df1.index = df1.index.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 株価を挿入する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>adj_close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>3265.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>3295.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>3380.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>3455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-11</th>\n",
       "      <td>3455.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-12</th>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-13</th>\n",
       "      <td>3535.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-14</th>\n",
       "      <td>3550.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-17</th>\n",
       "      <td>3500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-18</th>\n",
       "      <td>3510.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            adj_close\n",
       "2011-01-04     3265.0\n",
       "2011-01-05     3295.0\n",
       "2011-01-06     3380.0\n",
       "2011-01-07     3455.0\n",
       "2011-01-11     3455.0\n",
       "2011-01-12     3500.0\n",
       "2011-01-13     3535.0\n",
       "2011-01-14     3550.0\n",
       "2011-01-17     3500.0\n",
       "2011-01-18     3510.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 株価を取り出す\n",
    "df2 = pd.read_csv('./data/stock_price/7203.csv', index_col=0)\n",
    "df2.index = pd.to_datetime(df2['date'])\n",
    "df2.index = df2.index.date\n",
    "df2 = df2.drop(['date'], axis=1)\n",
    "df2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 時系列をくっつける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shogo/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: The join_axes-keyword is deprecated. Use .reindex or .reindex_like on the result to achieve the same functionality.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.concat([df1,df2], axis=1, join_axes=[df1.index])\n",
    "df3['price'] = np.round(df2.pct_change().shift(-1) * 100, 3)\n",
    "df3['Keyword_Article'] = \\\n",
    "    df3.groupby(level=0).apply(lambda x: ':<pad>:'.join(list(x['Keyword_Article'])))\n",
    "df3 = df3.dropna()\n",
    "\n",
    "df3 = df3[~df3.duplicated(subset=['Keyword_Article'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company_Code(TSE)</th>\n",
       "      <th>Headline</th>\n",
       "      <th>News_Source</th>\n",
       "      <th>Company_Relevance</th>\n",
       "      <th>Keyword_Article</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-04</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;日経&gt;◇次世代車の研究開発　名大に国内最大拠点</td>\n",
       "      <td>日経</td>\n",
       "      <td>38</td>\n",
       "      <td>安全:環境:負荷:開発:目指す:開所式:研究拠点:効率:簡素化:次世代:電気自動車:電気:幅...</td>\n",
       "      <td>3265.0</td>\n",
       "      <td>0.919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-05</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;日経&gt;◇12月の中国新車販売、トヨタが単月で過去最高</td>\n",
       "      <td>日経</td>\n",
       "      <td>100</td>\n",
       "      <td>北京:中国:１２月:新車販売台数:前年同月比:増:過去最高:制限:受け:全国:各地:乗用車:...</td>\n",
       "      <td>3295.0</td>\n",
       "      <td>2.580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-06</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;NQN&gt;◇トヨタ社長「今年は後半に晴れ間」　為替は１ドル＝90円を期待</td>\n",
       "      <td>ＮＱＮ</td>\n",
       "      <td>100</td>\n",
       "      <td>豊田:見通し:販売:エコカー補助金:安定的:伸び:株価:為替:水準:日経平均株価:最低:ライ...</td>\n",
       "      <td>3380.0</td>\n",
       "      <td>2.219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-07</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;日経&gt;◇福岡県、自動車の技術者育成へ新組織　年内、中小向け</td>\n",
       "      <td>日経</td>\n",
       "      <td>37</td>\n",
       "      <td>自動車産業:強化:福岡:先端:設置:方針:技術:調査:ニーズ:カリキュラム:大学:受け:生産...</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-11</th>\n",
       "      <td>7203.0</td>\n",
       "      <td>&lt;日経&gt;◇トヨタ、米ミシガン州に安全研究センター新設</td>\n",
       "      <td>日経</td>\n",
       "      <td>100</td>\n",
       "      <td>先進:安全:子供:高齢者:事故:向上:目指す:米国:大規模:リコール:回収:問題:開催:豊田...</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>1.302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Company_Code(TSE)                              Headline  \\\n",
       "2011-01-04             7203.0              <日経>◇次世代車の研究開発　名大に国内最大拠点   \n",
       "2011-01-05             7203.0           <日経>◇12月の中国新車販売、トヨタが単月で過去最高   \n",
       "2011-01-06             7203.0  <NQN>◇トヨタ社長「今年は後半に晴れ間」　為替は１ドル＝90円を期待   \n",
       "2011-01-07             7203.0        <日経>◇福岡県、自動車の技術者育成へ新組織　年内、中小向け   \n",
       "2011-01-11             7203.0            <日経>◇トヨタ、米ミシガン州に安全研究センター新設   \n",
       "\n",
       "           News_Source Company_Relevance  \\\n",
       "2011-01-04          日経                38   \n",
       "2011-01-05          日経               100   \n",
       "2011-01-06         ＮＱＮ               100   \n",
       "2011-01-07          日経                37   \n",
       "2011-01-11          日経               100   \n",
       "\n",
       "                                              Keyword_Article  adj_close  \\\n",
       "2011-01-04  安全:環境:負荷:開発:目指す:開所式:研究拠点:効率:簡素化:次世代:電気自動車:電気:幅...     3265.0   \n",
       "2011-01-05  北京:中国:１２月:新車販売台数:前年同月比:増:過去最高:制限:受け:全国:各地:乗用車:...     3295.0   \n",
       "2011-01-06  豊田:見通し:販売:エコカー補助金:安定的:伸び:株価:為替:水準:日経平均株価:最低:ライ...     3380.0   \n",
       "2011-01-07  自動車産業:強化:福岡:先端:設置:方針:技術:調査:ニーズ:カリキュラム:大学:受け:生産...     3455.0   \n",
       "2011-01-11  先進:安全:子供:高齢者:事故:向上:目指す:米国:大規模:リコール:回収:問題:開催:豊田...     3455.0   \n",
       "\n",
       "            price  \n",
       "2011-01-04  0.919  \n",
       "2011-01-05  2.580  \n",
       "2011-01-06  2.219  \n",
       "2011-01-07  0.000  \n",
       "2011-01-11  1.302  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csvファイルに保存する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_date = 2015\n",
    "test_date = 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.concat([df3[['Keyword_Article', 'price']].rename(\n",
    "                                      columns={'Keyword_Article': 'state', 'price': 'reward'}),\n",
    "                               df3[['Keyword_Article']].shift(-1).rename(\n",
    "                                      columns={'Keyword_Article': 'next_state'})], axis=1).dropna()\n",
    "df4 = df4[['state', 'next_state', 'reward']]\n",
    "\n",
    "date_year = df4.index.map(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[date_year <= train_date].to_csv(\n",
    "        './data/news/text_train.tsv',\n",
    "        header=None,\n",
    "        index=None,\n",
    "        sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[(train_date < date_year) & (date_year < test_date)].to_csv(\n",
    "        './data/news/text_val.tsv',\n",
    "        header=None,\n",
    "        index=None,\n",
    "        sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[test_date <= date_year].to_csv(\n",
    "        './data/news/text_test.tsv',\n",
    "        header=None,\n",
    "        index=None,\n",
    "        sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "def preprocessing_text(text):\n",
    "    # カンマ、ピリオド以外の記号をスペースに置換\n",
    "    for p in string.punctuation:\n",
    "        if (p == \".\") or (p == \",\") or (p == \":\") or (p == \"<\")or (p == \">\"):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, \" \")\n",
    "\n",
    "    # ピリオドなどの前後にはスペースを入れておく\n",
    "    text = text.replace(\".\", \" . \")\n",
    "    text = text.replace(\",\", \" , \")\n",
    "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
    "def tokenizer_punctuation(text):\n",
    "    return text.strip().split(':')\n",
    "\n",
    "# 前処理と分かち書きをまとめた関数を定義\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer_punctuation(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1000\n",
    "batch_size = 32\n",
    "\n",
    "# 読み込んだ内容に対して行う処理を定義\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, \n",
    "                            use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, \n",
    "                            init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/news', train='text_train.tsv',\n",
    "    format='tsv',\n",
    "    fields=[('Text1', TEXT), ('Text2', TEXT), ('Label', LABEL)])\n",
    "train_ds = train_ds[0]\n",
    "\n",
    "# japanese_fasttext_vectors = Vectors(name='./data/news/cc.ja.300.vec')\n",
    "TEXT.build_vocab(train_ds, \n",
    "#                  vectors=japanese_fasttext_vectors,\n",
    "                 min_freq=10)\n",
    "TEXT.vocab.freqs\n",
    "\n",
    "train_dl = torchtext.data.Iterator(\n",
    "    train_ds, batch_size=batch_size, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   2,  428,    5,  ...,    1,    1,    1],\n",
      "        [   2,  691,  936,  ...,    1,    1,    1],\n",
      "        [   2,   28,  144,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,    0, 1884,  ...,    1,    1,    1],\n",
      "        [   2,   81,   33,  ...,    1,    1,    1],\n",
      "        [   2,    7,    5,  ...,    1,    1,    1]]), tensor([ 24,  96,  18,  27, 228,  43,  52,  32, 213,  66,  39,  31, 230,  13,\n",
      "         72,  62, 110,  24,  76,  30, 143,  92,  78,  58, 185, 151,  29,  12,\n",
      "         55, 141, 368,  24]))\n",
      "(tensor([[   2,  399,  209,  ...,    1,    1,    1],\n",
      "        [   2,  241,  239,  ...,    1,    1,    1],\n",
      "        [   2, 1791, 1362,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,  234,    7,  ...,    1,    1,    1],\n",
      "        [   2,   28,  108,  ...,    1,    1,    1],\n",
      "        [   2,   28,  100,  ...,    1,    1,    1]]), tensor([ 54,  33,  20,  88, 196,  43,  17,  45, 159, 186,  78,  72, 198,  45,\n",
      "         95,  39,  64, 136,  15, 130,  46,  18, 106, 279, 209,  40,  30, 194,\n",
      "          8,  61, 185,  51]))\n",
      "tensor([ 1.2860, -0.2450,  0.3360, -0.9730, -2.1870, -2.5430, -0.1340, -0.5580,\n",
      "        -0.8100,  2.1260,  0.7800,  2.1450, -0.1350,  1.0850, -0.3110, -1.6160,\n",
      "        -0.7850, -1.2440, -0.9350, -1.8660, -0.4400, -0.1560, -0.2340, -0.5890,\n",
      "         0.1690, -1.3490,  0.1300,  0.1350, -2.4530, -1.8690, -0.4050,  0.5690])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_dl))\n",
    "print(batch.Text1)\n",
    "print(batch.Text2)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  691,  936,   58,  454,  922,  481,  480,   20, 1343,  168,  337,\n",
       "           4, 1164,    0,    0, 1543,   48,   49,  324,    4,  245,  624, 1033,\n",
       "         578,    0,  205,   13,   45, 2139,   10,    0,   77,  548,  179, 1880,\n",
       "           6,  783,  709,  368,  223,   25,  755,  534,    1,  452,   15,   54,\n",
       "           4,  100,   85,   39,   37,  183,  386,  462,   41,   51,   47,  502,\n",
       "         160,  370,  472,  511,   69,    1,   39,    4,   40,    4,  352,  161,\n",
       "         241,  111,  261,   12,  814,  450,    4,  170,    4,    4,    7,  518,\n",
       "         300,  307,   75,  618,  137, 1138,    4,  837,  253,  100,    7,    3,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.Text1[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モデル構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab.freqs)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "PAD_IDX = 1\n",
    "NUM_QUANTILE = 51\n",
    "GAMMA = 0.99\n",
    "cumulative_density = torch.FloatTensor(\n",
    "            (2 * np.arange(NUM_QUANTILE) + 1) / (2.0 * NUM_QUANTILE)).to(device)\n",
    "\n",
    "quantile_weight = 1.0 / NUM_QUANTILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = QRDQN(TEXT.vocab.vectors, VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS,\n",
    "                        FILTER_SIZES, PAD_IDX)\n",
    "\n",
    "target_model = QRDQN(TEXT.vocab.vectors, VOCAB_SIZE, EMBEDDING_DIM, N_FILTERS,\n",
    "                        FILTER_SIZES, PAD_IDX)\n",
    "\n",
    "model = model.to(device)\n",
    "target_model = target_model.to(device)\n",
    "\n",
    "target_model.load_state_dict(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 最適化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法\n",
    "learning_rate = 2.5e-4\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(scores, y):    \n",
    "    correct = (scores == y)\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()\n",
    "    return acc\n",
    "\n",
    "def huber(x):\n",
    "        cond = (x.abs() < 1.0).float().detach()\n",
    "        return 0.5 * x.pow(2) * cond + (x.abs() - 0.5) * (1.0 - cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## テスト\n",
    "batch = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----start----\n",
      "--------------------\n",
      "epoch: 0\n",
      "loss: 14.014793395996094\n",
      "epi_reward: 63.80099991336465\n",
      "neutrals: 522   buys: 497\n",
      "--------------------\n",
      "epoch: 1\n",
      "loss: 17.07994842529297\n",
      "epi_reward: 61.43800051882863\n",
      "neutrals: 499   buys: 520\n",
      "--------------------\n",
      "epoch: 2\n",
      "loss: 21.002803802490234\n",
      "epi_reward: 28.801000529900193\n",
      "neutrals: 530   buys: 489\n",
      "--------------------\n",
      "epoch: 3\n",
      "loss: 12.7841157913208\n",
      "epi_reward: 9.196000434458256\n",
      "neutrals: 502   buys: 517\n",
      "--------------------\n",
      "epoch: 4\n",
      "loss: 18.988779067993164\n",
      "epi_reward: 55.64699983596802\n",
      "neutrals: 525   buys: 494\n",
      "--------------------\n",
      "epoch: 5\n",
      "loss: 13.454113006591797\n",
      "epi_reward: 32.823000790551305\n",
      "neutrals: 505   buys: 514\n",
      "--------------------\n",
      "epoch: 6\n",
      "loss: 15.917472839355469\n",
      "epi_reward: 41.71599982306361\n",
      "neutrals: 506   buys: 513\n",
      "--------------------\n",
      "epoch: 7\n",
      "loss: 16.480304718017578\n",
      "epi_reward: -18.802000258117914\n",
      "neutrals: 507   buys: 512\n",
      "--------------------\n",
      "epoch: 8\n",
      "loss: 11.955123901367188\n",
      "epi_reward: 18.090001184493303\n",
      "neutrals: 536   buys: 483\n",
      "--------------------\n",
      "epoch: 9\n",
      "loss: 9.232050895690918\n",
      "epi_reward: 70.53299953974783\n",
      "neutrals: 533   buys: 486\n",
      "--------------------\n",
      "epoch: 10\n",
      "loss: 18.661876678466797\n",
      "epi_reward: 41.11800096929073\n",
      "neutrals: 493   buys: 526\n",
      "--------------------\n",
      "epoch: 11\n",
      "loss: 20.242650985717773\n",
      "epi_reward: 1.6850011851638556\n",
      "neutrals: 503   buys: 516\n",
      "--------------------\n",
      "epoch: 12\n",
      "loss: 15.984646797180176\n",
      "epi_reward: 10.04300020262599\n",
      "neutrals: 527   buys: 492\n",
      "--------------------\n",
      "epoch: 13\n",
      "loss: 12.891079902648926\n",
      "epi_reward: 71.12000045180321\n",
      "neutrals: 511   buys: 508\n",
      "--------------------\n",
      "epoch: 14\n",
      "loss: 12.323803901672363\n",
      "epi_reward: 55.28899905458093\n",
      "neutrals: 485   buys: 534\n",
      "--------------------\n",
      "epoch: 15\n",
      "loss: 15.797680854797363\n",
      "epi_reward: 31.794001672416925\n",
      "neutrals: 482   buys: 537\n",
      "--------------------\n",
      "epoch: 16\n",
      "loss: 12.532402038574219\n",
      "epi_reward: -3.872999679297209\n",
      "neutrals: 523   buys: 496\n",
      "--------------------\n",
      "epoch: 17\n",
      "loss: 11.426411628723145\n",
      "epi_reward: -8.01600082591176\n",
      "neutrals: 509   buys: 510\n",
      "--------------------\n",
      "epoch: 18\n",
      "loss: 11.06779956817627\n",
      "epi_reward: 80.362000644207\n",
      "neutrals: 496   buys: 523\n",
      "--------------------\n",
      "epoch: 19\n",
      "loss: 12.04754638671875\n",
      "epi_reward: 69.11500117182732\n",
      "neutrals: 487   buys: 532\n",
      "--------------------\n",
      "epoch: 20\n",
      "loss: 15.849230766296387\n",
      "epi_reward: 30.65199918858707\n",
      "neutrals: 509   buys: 510\n",
      "--------------------\n",
      "epoch: 21\n",
      "loss: 13.663861274719238\n",
      "epi_reward: 59.10600098781288\n",
      "neutrals: 506   buys: 513\n",
      "--------------------\n",
      "epoch: 22\n",
      "loss: 10.684844017028809\n",
      "epi_reward: 44.40999999269843\n",
      "neutrals: 554   buys: 465\n",
      "--------------------\n",
      "epoch: 23\n",
      "loss: 10.695650100708008\n",
      "epi_reward: 39.48300051689148\n",
      "neutrals: 509   buys: 510\n",
      "--------------------\n",
      "epoch: 24\n",
      "loss: 9.831920623779297\n",
      "epi_reward: 10.554001368582249\n",
      "neutrals: 523   buys: 496\n",
      "--------------------\n",
      "epoch: 25\n",
      "loss: 10.113275527954102\n",
      "epi_reward: 12.165999634191394\n",
      "neutrals: 511   buys: 508\n",
      "--------------------\n",
      "epoch: 26\n",
      "loss: 10.13884162902832\n",
      "epi_reward: 37.77799861878157\n",
      "neutrals: 497   buys: 522\n",
      "--------------------\n",
      "epoch: 27\n",
      "loss: 10.705804824829102\n",
      "epi_reward: 66.61300135403872\n",
      "neutrals: 541   buys: 478\n",
      "--------------------\n",
      "epoch: 28\n",
      "loss: 9.914958000183105\n",
      "epi_reward: 40.90800115093589\n",
      "neutrals: 504   buys: 515\n",
      "--------------------\n",
      "epoch: 29\n",
      "loss: 10.394941329956055\n",
      "epi_reward: 65.67100016213953\n",
      "neutrals: 513   buys: 506\n",
      "--------------------\n",
      "epoch: 30\n",
      "loss: 18.764760971069336\n",
      "epi_reward: 28.26900132931769\n",
      "neutrals: 496   buys: 523\n",
      "--------------------\n",
      "epoch: 31\n",
      "loss: 10.788382530212402\n",
      "epi_reward: 16.393999492749572\n",
      "neutrals: 515   buys: 504\n",
      "--------------------\n",
      "epoch: 32\n",
      "loss: 9.04235553741455\n",
      "epi_reward: 57.094000443816185\n",
      "neutrals: 479   buys: 540\n",
      "--------------------\n",
      "epoch: 33\n",
      "loss: 8.493903160095215\n",
      "epi_reward: 43.47299937158823\n",
      "neutrals: 491   buys: 528\n",
      "--------------------\n",
      "epoch: 34\n",
      "loss: 8.673095703125\n",
      "epi_reward: 33.04199969023466\n",
      "neutrals: 516   buys: 503\n",
      "--------------------\n",
      "epoch: 35\n",
      "loss: 8.49905776977539\n",
      "epi_reward: 89.05099975503981\n",
      "neutrals: 495   buys: 524\n",
      "--------------------\n",
      "epoch: 36\n",
      "loss: 8.89873218536377\n",
      "epi_reward: 76.36300185509026\n",
      "neutrals: 514   buys: 505\n",
      "--------------------\n",
      "epoch: 37\n",
      "loss: 8.355862617492676\n",
      "epi_reward: 24.95100062713027\n",
      "neutrals: 530   buys: 489\n",
      "--------------------\n",
      "epoch: 38\n",
      "loss: 8.876185417175293\n",
      "epi_reward: 46.386000007390976\n",
      "neutrals: 494   buys: 525\n",
      "--------------------\n",
      "epoch: 39\n",
      "loss: 8.990653038024902\n",
      "epi_reward: 65.33300005272031\n",
      "neutrals: 532   buys: 487\n",
      "--------------------\n",
      "epoch: 40\n",
      "loss: 17.788217544555664\n",
      "epi_reward: 12.156999150291085\n",
      "neutrals: 516   buys: 503\n",
      "--------------------\n",
      "epoch: 41\n",
      "loss: 9.973687171936035\n",
      "epi_reward: 15.718001049011946\n",
      "neutrals: 518   buys: 501\n",
      "--------------------\n",
      "epoch: 42\n",
      "loss: 7.6545867919921875\n",
      "epi_reward: 24.06700070388615\n",
      "neutrals: 522   buys: 497\n",
      "--------------------\n",
      "epoch: 43\n",
      "loss: 7.626006126403809\n",
      "epi_reward: 25.44600093178451\n",
      "neutrals: 532   buys: 487\n",
      "--------------------\n",
      "epoch: 44\n",
      "loss: 7.4739580154418945\n",
      "epi_reward: 56.59800083376467\n",
      "neutrals: 517   buys: 502\n",
      "--------------------\n",
      "epoch: 45\n",
      "loss: 7.331669807434082\n",
      "epi_reward: 12.335000840947032\n",
      "neutrals: 514   buys: 505\n",
      "--------------------\n",
      "epoch: 46\n",
      "loss: 7.7864885330200195\n",
      "epi_reward: 22.62999987602234\n",
      "neutrals: 516   buys: 503\n",
      "--------------------\n",
      "epoch: 47\n",
      "loss: 7.255364418029785\n",
      "epi_reward: 31.76199990697205\n",
      "neutrals: 514   buys: 505\n",
      "--------------------\n",
      "epoch: 48\n",
      "loss: 7.63311767578125\n",
      "epi_reward: 34.151999646797776\n",
      "neutrals: 514   buys: 505\n",
      "--------------------\n",
      "epoch: 49\n",
      "loss: 7.113835334777832\n",
      "epi_reward: 34.43300030939281\n",
      "neutrals: 518   buys: 501\n",
      "--------------------\n",
      "epoch: 50\n",
      "loss: 11.817100524902344\n",
      "epi_reward: 68.24499976448715\n",
      "neutrals: 519   buys: 500\n",
      "--------------------\n",
      "epoch: 51\n",
      "loss: 7.28403377532959\n",
      "epi_reward: 53.72800062596798\n",
      "neutrals: 529   buys: 490\n",
      "--------------------\n",
      "epoch: 52\n",
      "loss: 6.943918228149414\n",
      "epi_reward: 84.29800106585026\n",
      "neutrals: 497   buys: 522\n",
      "--------------------\n",
      "epoch: 53\n",
      "loss: 6.528655052185059\n",
      "epi_reward: 43.822000816464424\n",
      "neutrals: 492   buys: 527\n",
      "--------------------\n",
      "epoch: 54\n",
      "loss: 6.505197048187256\n",
      "epi_reward: 47.67200058884919\n",
      "neutrals: 521   buys: 498\n",
      "--------------------\n",
      "epoch: 55\n",
      "loss: 6.552074432373047\n",
      "epi_reward: 100.96200010553002\n",
      "neutrals: 523   buys: 496\n",
      "--------------------\n",
      "epoch: 56\n",
      "loss: 6.496609210968018\n",
      "epi_reward: 88.90900065377355\n",
      "neutrals: 513   buys: 506\n",
      "--------------------\n",
      "epoch: 57\n",
      "loss: 6.186193943023682\n",
      "epi_reward: 57.076001573354006\n",
      "neutrals: 511   buys: 508\n",
      "--------------------\n",
      "epoch: 58\n",
      "loss: 6.279881477355957\n",
      "epi_reward: 77.58300076797605\n",
      "neutrals: 517   buys: 502\n",
      "--------------------\n",
      "epoch: 59\n",
      "loss: 6.576070308685303\n",
      "epi_reward: 54.08299995586276\n",
      "neutrals: 503   buys: 516\n",
      "--------------------\n",
      "epoch: 60\n",
      "loss: 17.969406127929688\n",
      "epi_reward: 12.779000604525208\n",
      "neutrals: 495   buys: 524\n",
      "--------------------\n",
      "epoch: 61\n",
      "loss: 7.642069339752197\n",
      "epi_reward: 90.93800155259669\n",
      "neutrals: 505   buys: 514\n",
      "--------------------\n",
      "epoch: 62\n",
      "loss: 6.5075154304504395\n",
      "epi_reward: 28.67000007815659\n",
      "neutrals: 493   buys: 526\n",
      "--------------------\n",
      "epoch: 63\n",
      "loss: 5.74647855758667\n",
      "epi_reward: 84.56200053729117\n",
      "neutrals: 513   buys: 506\n",
      "--------------------\n",
      "epoch: 64\n",
      "loss: 5.825347423553467\n",
      "epi_reward: 41.764999728649855\n",
      "neutrals: 507   buys: 512\n",
      "--------------------\n",
      "epoch: 65\n",
      "loss: 5.351293563842773\n",
      "epi_reward: 35.13200074248016\n",
      "neutrals: 506   buys: 513\n",
      "--------------------\n",
      "epoch: 66\n",
      "loss: 5.368490695953369\n",
      "epi_reward: 42.153001027181745\n",
      "neutrals: 497   buys: 522\n",
      "--------------------\n",
      "epoch: 67\n",
      "loss: 5.289182186126709\n",
      "epi_reward: 91.12300114519894\n",
      "neutrals: 502   buys: 517\n",
      "--------------------\n",
      "epoch: 68\n",
      "loss: 5.512369632720947\n",
      "epi_reward: 105.42100064642727\n",
      "neutrals: 520   buys: 499\n",
      "--------------------\n",
      "epoch: 69\n",
      "loss: 5.891641139984131\n",
      "epi_reward: 5.5410017389804125\n",
      "neutrals: 503   buys: 516\n",
      "--------------------\n",
      "epoch: 70\n",
      "loss: 13.982156753540039\n",
      "epi_reward: 16.747000528499484\n",
      "neutrals: 527   buys: 492\n",
      "--------------------\n",
      "epoch: 71\n",
      "loss: 6.459866046905518\n",
      "epi_reward: 3.667999379336834\n",
      "neutrals: 501   buys: 518\n",
      "--------------------\n",
      "epoch: 72\n",
      "loss: 5.242201805114746\n",
      "epi_reward: -6.225998964160681\n",
      "neutrals: 521   buys: 498\n",
      "--------------------\n",
      "epoch: 73\n",
      "loss: 5.376680374145508\n",
      "epi_reward: 25.88800153322518\n",
      "neutrals: 506   buys: 513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "epoch: 74\n",
      "loss: 5.730113983154297\n",
      "epi_reward: 28.474000414833426\n",
      "neutrals: 490   buys: 529\n",
      "--------------------\n",
      "epoch: 75\n",
      "loss: 4.744917392730713\n",
      "epi_reward: 73.12600051425397\n",
      "neutrals: 518   buys: 501\n",
      "--------------------\n",
      "epoch: 76\n",
      "loss: 4.712222576141357\n",
      "epi_reward: 8.863999167457223\n",
      "neutrals: 505   buys: 514\n",
      "--------------------\n",
      "epoch: 77\n",
      "loss: 4.894913196563721\n",
      "epi_reward: 105.33100029453635\n",
      "neutrals: 492   buys: 527\n",
      "--------------------\n",
      "epoch: 78\n",
      "loss: 4.6082916259765625\n",
      "epi_reward: 4.261001167818904\n",
      "neutrals: 502   buys: 517\n",
      "--------------------\n",
      "epoch: 79\n",
      "loss: 4.863260269165039\n",
      "epi_reward: 66.32999934628606\n",
      "neutrals: 484   buys: 535\n",
      "--------------------\n",
      "epoch: 80\n",
      "loss: 12.57967472076416\n",
      "epi_reward: 35.88699993863702\n",
      "neutrals: 553   buys: 466\n",
      "--------------------\n",
      "epoch: 81\n",
      "loss: 4.895755290985107\n",
      "epi_reward: 22.752000629901886\n",
      "neutrals: 506   buys: 513\n",
      "--------------------\n",
      "epoch: 82\n",
      "loss: 4.482987403869629\n",
      "epi_reward: 22.77899995073676\n",
      "neutrals: 515   buys: 504\n",
      "--------------------\n",
      "epoch: 83\n",
      "loss: 4.954736709594727\n",
      "epi_reward: 7.803000662475824\n",
      "neutrals: 483   buys: 536\n",
      "--------------------\n",
      "epoch: 84\n",
      "loss: 4.360623359680176\n",
      "epi_reward: 31.81600067205727\n",
      "neutrals: 523   buys: 496\n",
      "--------------------\n",
      "epoch: 85\n",
      "loss: 4.567389488220215\n",
      "epi_reward: 17.514999551698565\n",
      "neutrals: 527   buys: 492\n",
      "--------------------\n",
      "epoch: 86\n",
      "loss: 4.363265514373779\n",
      "epi_reward: 7.666999204084277\n",
      "neutrals: 522   buys: 497\n",
      "--------------------\n",
      "epoch: 87\n",
      "loss: 3.905726194381714\n",
      "epi_reward: 2.4669998306781054\n",
      "neutrals: 512   buys: 507\n",
      "--------------------\n",
      "epoch: 88\n",
      "loss: 3.9512574672698975\n",
      "epi_reward: 48.89900113083422\n",
      "neutrals: 501   buys: 518\n",
      "--------------------\n",
      "epoch: 89\n",
      "loss: 4.128592491149902\n",
      "epi_reward: 85.15599978156388\n",
      "neutrals: 511   buys: 508\n",
      "--------------------\n",
      "epoch: 90\n",
      "loss: 16.38986587524414\n",
      "epi_reward: 56.62400173768401\n",
      "neutrals: 499   buys: 520\n",
      "--------------------\n",
      "epoch: 91\n",
      "loss: 6.949010372161865\n",
      "epi_reward: 28.2790010496974\n",
      "neutrals: 493   buys: 526\n",
      "--------------------\n",
      "epoch: 92\n",
      "loss: 6.074441909790039\n",
      "epi_reward: 98.20700043439865\n",
      "neutrals: 516   buys: 503\n",
      "--------------------\n",
      "epoch: 93\n",
      "loss: 4.4958014488220215\n",
      "epi_reward: 65.87899961136281\n",
      "neutrals: 504   buys: 515\n",
      "--------------------\n",
      "epoch: 94\n",
      "loss: 4.028349876403809\n",
      "epi_reward: 37.019999612122774\n",
      "neutrals: 509   buys: 510\n",
      "--------------------\n",
      "epoch: 95\n",
      "loss: 3.6817119121551514\n",
      "epi_reward: 48.710999539121985\n",
      "neutrals: 510   buys: 509\n",
      "--------------------\n",
      "epoch: 96\n",
      "loss: 4.203797340393066\n",
      "epi_reward: 74.31899883970618\n",
      "neutrals: 526   buys: 493\n",
      "--------------------\n",
      "epoch: 97\n",
      "loss: 4.441027641296387\n",
      "epi_reward: 0.7890016529709101\n",
      "neutrals: 510   buys: 509\n",
      "--------------------\n",
      "epoch: 98\n",
      "loss: 4.984102249145508\n",
      "epi_reward: 63.66200091317296\n",
      "neutrals: 505   buys: 514\n",
      "--------------------\n",
      "epoch: 99\n",
      "loss: 3.9545016288757324\n",
      "epi_reward: 77.69800028577447\n",
      "neutrals: 497   buys: 522\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "TARGET_UPDATE_FREQ = 10\n",
    "# dataloaders_dict = {'train': train_dl, 'val':val_dl}\n",
    "dataloaders_dict = {'train': train_dl}\n",
    "\n",
    "print('----start----')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epi_rewards = []\n",
    "    neutrals = []\n",
    "    buys = []\n",
    "    \n",
    "    # update target_model\n",
    "    if epoch % TARGET_UPDATE_FREQ == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "    \n",
    "    for batch in (dataloaders_dict['train']):      \n",
    "        # curr_q\n",
    "        states = batch.Text1[0].to(device)\n",
    "        next_states = batch.Text2[0].to(device)\n",
    "        rewards = batch.Label.to(device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            dist = model(states) * quantile_weight\n",
    "            actions = dist.sum(dim=2).max(1)[1]\n",
    "\n",
    "            actions = torch.where(torch.randn(len(states)).to(device) >= 0, \n",
    "                                                   actions, \n",
    "                                                  (actions + 1) % 2)\n",
    "\n",
    "            selected_actions = actions.detach().cpu().numpy()\n",
    "            actions = actions.view(states.size(0), 1, 1).expand(-1, -1, NUM_QUANTILE)\n",
    "\n",
    "\n",
    "        epi_rewards.append((selected_actions * rewards.detach().cpu().numpy()).sum())\n",
    "        neutrals.append(len(selected_actions[selected_actions == 0]))\n",
    "        buys.append(len(selected_actions[selected_actions == 1]))\n",
    "        \n",
    "        curr_q = model(states).gather(1, actions).squeeze(dim=1)\n",
    "\n",
    "        # target_q\n",
    "        with torch.no_grad():\n",
    "\n",
    "            next_dist = model(next_states) * quantile_weight\n",
    "            next_action = next_dist.sum(dim=2).max(1)[1].view(next_states.size(0), 1, 1).expand(\n",
    "                -1, -1, NUM_QUANTILE)\n",
    "\n",
    "            next_q = target_model(next_states).gather(1, next_action).squeeze(dim=1)\n",
    "            target_q = rewards.view(-1, 1) + (GAMMA * next_q)\n",
    "\n",
    "        diff = target_q.t().unsqueeze(-1) - curr_q.unsqueeze(0)\n",
    "        loss = huber(diff) * torch.abs(\n",
    "                        cumulative_density.view(1, -1) - (diff < 0).to(torch.float))\n",
    "        loss = loss.transpose(0, 1)\n",
    "        loss = loss.mean(1).sum(-1).mean()\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        optimizer.step()\n",
    "    \n",
    "    print('--------------------')\n",
    "    print('epoch:', epoch)\n",
    "    print('loss:', loss.item())\n",
    "    print('epi_reward:', sum(epi_rewards))\n",
    "    print('neutrals:', sum(neutrals), '  buys:', sum(buys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "states = batch.Text1[0].to(device)\n",
    "next_states = batch.Text2[0].to(device)\n",
    "rewards = batch.Label.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = states[5]\n",
    "actions = (model(state.unsqueeze(0)) * quantile_weight)\n",
    "dist_action = actions[0].cpu().detach().numpy()\n",
    "# sns.distplot(dist_action[0], bins=51, color='red')\n",
    "sns.distplot(dist_action[1], bins=10, color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04582909, -0.01779904, -0.02187311, -0.02663671, -0.01788767,\n",
       "       -0.02439825, -0.01547612, -0.01577615, -0.01202005, -0.01723998,\n",
       "       -0.01691859, -0.01512901, -0.01630607, -0.01729473, -0.01451513,\n",
       "       -0.0156123 , -0.01525346, -0.00912168, -0.01328274, -0.00922554,\n",
       "       -0.01297883, -0.0107121 , -0.01250854, -0.01094983, -0.01092977,\n",
       "       -0.01107626, -0.00696504, -0.00584164, -0.00657483, -0.00743093,\n",
       "       -0.00876693, -0.01085366, -0.00690597, -0.00548392, -0.00878991,\n",
       "       -0.00513259, -0.00290729, -0.00398217, -0.00598274, -0.00460264,\n",
       "       -0.00362856, -0.00496699, -0.00592583, -0.00466383, -0.00071197,\n",
       "        0.00171334, -0.00509513,  0.00149893,  0.00134867,  0.00388506,\n",
       "        0.01655775], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_action[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
