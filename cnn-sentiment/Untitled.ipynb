{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 単語分割\n",
    "\n",
    "# 前処理\n",
    "\n",
    "# 文章とラベルに分ける\n",
    "\n",
    "# データを読み込む\n",
    "\n",
    "# ボキャブラリーを作成\n",
    "\n",
    "# DataLoaderの作成\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単語分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "m_t = MeCab.Tagger('-Ochasen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '機械学習が好きです'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "機械\tキカイ\t機械\t名詞-一般\t\t\n",
      "学習\tガクシュウ\t学習\t名詞-サ変接続\t\t\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "好き\tスキ\t好き\t名詞-形容動詞語幹\t\t\n",
      "です\tデス\tです\t助動詞\t特殊・デス\t基本形\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "print(m_t.parse(text).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_mecab(text):\n",
    "    text = m_t.parse(text) # mecab\n",
    "    ret = text.strip().split()  # text処理\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # 前処理\n",
    "    text = re.sub('\\r', '', text) # \n",
    "    text = re.sub('\\n', '', text) # 改行\n",
    "    text = re.sub(' ', '', text) # 半角\n",
    "    text = re.sub('　　', '', text) # 全角\n",
    "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizerの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text) # textの前処理\n",
    "    ret = tokenizer_mecab(text) # mecabの単語分割\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "max_length = 25\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing,\n",
    "                                                 use_vocab=True, lower=True, include_lengths=True, \n",
    "                                                 batch_first=True, fix_length=max_length)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasetの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='text_train.tsv',\n",
    "    validation='text_val.tsv', test='text_test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': ['王',\n",
       "  'オウ',\n",
       "  '王',\n",
       "  '名詞-一般',\n",
       "  'と',\n",
       "  'ト',\n",
       "  'と',\n",
       "  '助詞-並立助詞',\n",
       "  '王子',\n",
       "  'オウジ',\n",
       "  '王子',\n",
       "  '名詞-一般',\n",
       "  'と',\n",
       "  'ト',\n",
       "  'と',\n",
       "  '助詞-並立助詞',\n",
       "  '女王',\n",
       "  'ジョオウ',\n",
       "  '女王',\n",
       "  '名詞-一般',\n",
       "  'と',\n",
       "  'ト',\n",
       "  'と',\n",
       "  '助詞-並立助詞',\n",
       "  '姫',\n",
       "  'ヒメ',\n",
       "  '姫',\n",
       "  '名詞-一般',\n",
       "  'と',\n",
       "  'ト',\n",
       "  'と',\n",
       "  '助詞-並立助詞',\n",
       "  '男性',\n",
       "  'ダンセイ',\n",
       "  '男性',\n",
       "  '名詞-一般',\n",
       "  'と',\n",
       "  'ト',\n",
       "  'と',\n",
       "  '助詞-並立助詞',\n",
       "  '女性',\n",
       "  'ジョセイ',\n",
       "  '女性',\n",
       "  '名詞-一般',\n",
       "  'が',\n",
       "  'ガ',\n",
       "  'が',\n",
       "  '助詞-格助詞-一般',\n",
       "  'い',\n",
       "  'イ',\n",
       "  'いる',\n",
       "  '動詞-自立',\n",
       "  '一段',\n",
       "  '連用形',\n",
       "  'まし',\n",
       "  'マシ',\n",
       "  'ます',\n",
       "  '助動詞',\n",
       "  '特殊・マス',\n",
       "  '連用形',\n",
       "  'た',\n",
       "  'タ',\n",
       "  'た',\n",
       "  '助動詞',\n",
       "  '特殊・タ',\n",
       "  '基本形',\n",
       "  '。',\n",
       "  '。',\n",
       "  '。',\n",
       "  '記号-句点',\n",
       "  'eos'],\n",
       " 'Label': '0'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ボキャブラリーの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'王': 2,\n",
       "         'オウ': 1,\n",
       "         '名詞-一般': 17,\n",
       "         'と': 10,\n",
       "         'ト': 5,\n",
       "         '助詞-並立助詞': 5,\n",
       "         '王子': 2,\n",
       "         'オウジ': 1,\n",
       "         '女王': 2,\n",
       "         'ジョオウ': 1,\n",
       "         '姫': 2,\n",
       "         'ヒメ': 1,\n",
       "         '男性': 2,\n",
       "         'ダンセイ': 1,\n",
       "         '女性': 2,\n",
       "         'ジョセイ': 1,\n",
       "         'が': 6,\n",
       "         'ガ': 3,\n",
       "         '助詞-格助詞-一般': 10,\n",
       "         'い': 1,\n",
       "         'イ': 1,\n",
       "         'いる': 5,\n",
       "         '動詞-自立': 6,\n",
       "         '一段': 3,\n",
       "         '連用形': 6,\n",
       "         'まし': 1,\n",
       "         'マシ': 1,\n",
       "         'ます': 5,\n",
       "         '助動詞': 9,\n",
       "         '特殊・マス': 3,\n",
       "         'た': 2,\n",
       "         'タ': 1,\n",
       "         '特殊・タ': 1,\n",
       "         '基本形': 8,\n",
       "         '。': 12,\n",
       "         '記号-句点': 4,\n",
       "         'eos': 4,\n",
       "         '機械': 2,\n",
       "         'キカイ': 1,\n",
       "         '学習': 2,\n",
       "         'ガクシュウ': 1,\n",
       "         '名詞-サ変接続': 8,\n",
       "         '好き': 2,\n",
       "         'スキ': 1,\n",
       "         '名詞-形容動詞語幹': 4,\n",
       "         'です': 2,\n",
       "         'デス': 1,\n",
       "         '特殊・デス': 1,\n",
       "         '本章': 4,\n",
       "         'ホンショウ': 2,\n",
       "         'から': 2,\n",
       "         'カラ': 1,\n",
       "         '自然': 2,\n",
       "         'シゼン': 1,\n",
       "         '言語': 2,\n",
       "         'ゲンゴ': 1,\n",
       "         '処理': 2,\n",
       "         'ショリ': 1,\n",
       "         'に': 2,\n",
       "         'ニ': 1,\n",
       "         '取り組み': 1,\n",
       "         'トリクミ': 1,\n",
       "         '取り組む': 1,\n",
       "         '五段・マ行': 1,\n",
       "         'マス': 2,\n",
       "         'で': 2,\n",
       "         'デ': 1,\n",
       "         'は': 2,\n",
       "         'ハ': 1,\n",
       "         '助詞-係助詞': 1,\n",
       "         '商品': 2,\n",
       "         'ショウヒン': 1,\n",
       "         'レビュー': 3,\n",
       "         'の': 8,\n",
       "         'ノ': 4,\n",
       "         '短い': 2,\n",
       "         'ミジカイ': 1,\n",
       "         '形容詞-自立': 1,\n",
       "         '形容詞・アウオ段': 1,\n",
       "         '文章': 8,\n",
       "         'ブンショウ': 4,\n",
       "         'に対して': 2,\n",
       "         'ニタイシテ': 1,\n",
       "         '助詞-格助詞-連語': 1,\n",
       "         '、': 9,\n",
       "         '記号-読点': 3,\n",
       "         'その': 2,\n",
       "         'ソノ': 1,\n",
       "         '連体詞': 1,\n",
       "         'ネガティブ': 3,\n",
       "         'な': 4,\n",
       "         'ナ': 4,\n",
       "         'だ': 4,\n",
       "         '特殊・ダ': 4,\n",
       "         '体言接続': 4,\n",
       "         '評価': 4,\n",
       "         'ヒョウカ': 2,\n",
       "         'を': 6,\n",
       "         'ヲ': 3,\n",
       "         'し': 3,\n",
       "         'シ': 3,\n",
       "         'する': 5,\n",
       "         'サ変・スル': 4,\n",
       "         'て': 4,\n",
       "         'テ': 2,\n",
       "         '助詞-接続助詞': 2,\n",
       "         'イル': 2,\n",
       "         '動詞-非自立': 2,\n",
       "         '名詞-非自立-一般': 2,\n",
       "         'か': 4,\n",
       "         'カ': 2,\n",
       "         '助詞-副助詞／並立助詞／終助詞': 2,\n",
       "         'ポジティブ': 3,\n",
       "         '0': 3,\n",
       "         '名詞-数': 1,\n",
       "         '値': 2,\n",
       "         'チ': 1,\n",
       "         '名詞-接尾-一般': 1,\n",
       "         '助詞-連体化': 1,\n",
       "         'クラス': 3,\n",
       "         '分類': 4,\n",
       "         'ブンルイ': 2,\n",
       "         'スル': 1,\n",
       "         'モデル': 3,\n",
       "         '構築': 2,\n",
       "         'コウチク': 1})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(train_ds, min_freq=1)\n",
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl= torchtext.data.Iterator(train_ds, batch_size=2, train=True)\n",
    "val_dl = torchtext.data.Iterator(val_ds, batch_size=2, train=False, sort=False)\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=2, train=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 33,  63,  33,   2,  50,  90,  50,   5,  83,  95,  83,  32,  84,  93,\n",
      "          84,   2,  66,  97,  66,   9,  55, 109,  55,   5, 119],\n",
      "        [ 33,  63,  33,   2,  53, 106,  53,   5,  57, 111,  57, 116,  71,  96,\n",
      "          71,   2,  45,  45,  45,   9,   8,  28,   8,   5,  82]]), tensor([25, 25]))\n",
      "tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "batch = next(iter(train_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDbデータセットの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 訓練データの作成\n",
    "with open('./data/IMDb_train.tsv', 'w') as f:\n",
    "\n",
    "    path = './data/aclImdb/train/pos/'\n",
    "    for fname in glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding='utf-8') as ff:\n",
    "            text = ff.readline()\n",
    "            text = text.replace('\\t', ' ')\n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "\n",
    "    path = './data/aclImdb/train/neg/'\n",
    "    for fname in glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding='utf-8') as ff:\n",
    "            text = ff.readline()\n",
    "            text = text.replace('\\t', ' ')\n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの作成\n",
    "with open('./data/IMDb_test.tsv', 'w') as f:\n",
    "\n",
    "    path = './data/aclImdb/test/pos/'\n",
    "    for fname in glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding='utf-8') as ff:\n",
    "            text = ff.readline()\n",
    "            text = text.replace('\\t', ' ')\n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    path = './data/aclImdb/test/neg/'\n",
    "    for fname in glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding='utf-8') as ff:\n",
    "            text = ff.readline()\n",
    "            text = text.replace('\\t', ' ')\n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# 前処理\n",
    "def preprocessing_text(text):\n",
    "    text = re.sub('<br />', '', text)\n",
    "    \n",
    "    # カンマ・ピリオド以外の記号をスペースに変換\n",
    "    for p in string.punctuation:\n",
    "        if (p =='.') or (p == ','):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, ' ')\n",
    "        \n",
    "    # ピリオドの前後にはスペースを入れる\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace(',', ' , ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 分かち書き\n",
    "def tokenizer_punctuation(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "# 前処理と分かち書きをまとめる\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer_punctuation(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'lick', 'cats', '.']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_with_preprocessing('I lick cats.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "max_length = 256\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='IMDb_train.tsv',\n",
    "    test='IMDb_test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text': ['very', 'different', 'topic', 'treated', 'in', 'this', 'film', '.', 'a', 'straightforward', 'and', 'simple', 'description', 'of', 'local', 'chinese', 'customs', ',', 'by', 'looking', 'at', 'the', 'daily', 'operation', 'of', 'a', 'public', 'bath', ',', 'run', 'by', 'the', 'old', 'owner', 'and', 'his', 'retarded', 'son', ',', 'when', 'older', 'son', 'returns', 'home', ',', 'wrongly', 'believing', 'his', 'father', 'has', 'died', '.', 'how', 'every', 'man', 'in', 'town', 'makes', 'his', 'daily', 'visit', 'to', 'chat', ',', 'play', 'games', ',', 'discuss', 'personal', 'matters', 'and', 'get', 'honest', 'advice', ',', 'besides', 'the', 'usual', 'spa', 'like', 'therapies', '.', 'when', 'old', 'man', 'dies', ',', 'strong', 'and', 'loyal', 'family', 'ties', 'make', 'older', 'son', 'take', 'charge', ',', 'so', 'public', 'bath', 'operation', 'is', 'not', 'disrupted', '.', 'and', 'finally', ',', 'the', 'arrival', 'of', 'modernization', 'to', 'end', 'this', 'way', 'of', 'spending', 'relaxed', 'hours', 'and', 'getting', 'along', '.', 'the', 'public', 'bath', 'has', 'to', 'be', 'demolished', ',', 'making', 'place', 'for', 'a', 'commercial', 'complex', 'to', 'be', 'constructed', '.'], 'Label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_val_ds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練と検証を分ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ボキャブラリーの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no vectors found at .vector_cache/./data/wiki-news-300d-1M.vec",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-26f000d816e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menglish_fasttext_vocabs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data/wiki-news-300d-1M.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    370\u001b[0m                             \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no vectors found at {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading vectors from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no vectors found at .vector_cache/./data/wiki-news-300d-1M.vec"
     ]
    }
   ],
   "source": [
    "english_fasttext_vocabs = Vectors(name='./data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語の次元数 300\n",
      "単語数 999994\n"
     ]
    }
   ],
   "source": [
    "print('単語の次元数', english_fasttext_vocabs.dim)\n",
    "print('単語数', len(english_fasttext_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_ds, vectors=english_fasttext_vocabs, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f2c20444c90>>, {'<unk>': 0, '<pad>': 1, '<cls>': 2, '<eos>': 3})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torchtext.data.Iterator(train_ds, batch_size=24, train=True)\n",
    "val_dl = torchtext.data.Iterator(val_ds, batch_size=24, train=False, sort=False)\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=24, train=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   2,   47,  223,  ...,  133, 1742,    3],\n",
      "        [   2,   52,   55,  ...,    1,    1,    1],\n",
      "        [   2,   14,  146,  ...,    1,    1,    1],\n",
      "        ...,\n",
      "        [   2,    5,    5,  ...,    1,    1,    1],\n",
      "        [   2, 3089, 2184,  ...,  447,  285,    3],\n",
      "        [   2,    4,  687,  ...,  345,    7,    3]]), tensor([256, 103, 174, 174, 202, 256, 204, 256, 153, 119, 256, 256, 132, 256,\n",
      "         57, 256, 256, 143, 256, 256, 159, 211, 256, 256]))\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedder (単語数　→ 単語数*分散表現数)\n",
    "# Positional Encoder 単語数*分散表現数の位置情報を加える(単語数*分散表現数 → 単語数*分散表現数)\n",
    "# Transformer Blockモジュール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, text_embedding_vectors):\n",
    "        super(Embedder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(\n",
    "        embeddings=text_embedding_vectors, freeze=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.embeddings(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     11
    ]
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "\n",
    "def get_IMDb_DataLoaders_and_TEXT(max_length=256, batch_size=24):\n",
    "    \"\"\"IMDbのDataLoaderとTEXTオブジェクトを取得する。 \"\"\"\n",
    "\n",
    "    # 訓練データのtsvファイルを作成します\n",
    "    f = open('./data/IMDb_train.tsv', 'w')\n",
    "\n",
    "    path = './data/aclImdb/train/pos/'\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "\n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "\n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    path = './data/aclImdb/train/neg/'\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "\n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "\n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "   # テストデータの作成\n",
    "    f = open('./data/IMDb_test.tsv', 'w')\n",
    "\n",
    "    path = './data/aclImdb/test/pos/'\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "\n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "\n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    path = './data/aclImdb/test/neg/'\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "\n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "\n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "    f.close()\n",
    "\n",
    "    def preprocessing_text(text):\n",
    "        # 改行コードを消去\n",
    "        text = re.sub('<br />', '', text)\n",
    "\n",
    "        # カンマ、ピリオド以外の記号をスペースに置換\n",
    "        for p in string.punctuation:\n",
    "            if (p == \".\") or (p == \",\"):\n",
    "                continue\n",
    "            else:\n",
    "                text = text.replace(p, \" \")\n",
    "\n",
    "        # ピリオドなどの前後にはスペースを入れておく\n",
    "        text = text.replace(\".\", \" . \")\n",
    "        text = text.replace(\",\", \" , \")\n",
    "        return text\n",
    "\n",
    "    # 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
    "    def tokenizer_punctuation(text):\n",
    "        return text.strip().split()\n",
    "\n",
    "\n",
    "    # 前処理と分かち書きをまとめた関数を定義\n",
    "    def tokenizer_with_preprocessing(text):\n",
    "        text = preprocessing_text(text)\n",
    "        ret = tokenizer_punctuation(text)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    # データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "    # max_length\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                                lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "    LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "    # フォルダ「data」から各tsvファイルを読み込みます\n",
    "    train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "        path='./data/', train='IMDb_train.tsv',\n",
    "        test='IMDb_test.tsv', format='tsv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "    # torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "    train_ds, val_ds = train_val_ds.split(\n",
    "        split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "    # torchtextで単語ベクトルとして英語学習済みモデルを読み込みます\n",
    "    english_fasttext_vectors = Vectors(name='data/wiki-news-300d-1M.vec')\n",
    "\n",
    "    # ベクトル化したバージョンのボキャブラリーを作成します\n",
    "    TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
    "\n",
    "    # DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "    train_dl = torchtext.data.Iterator(\n",
    "        train_ds, batch_size=batch_size, train=True)\n",
    "\n",
    "    val_dl = torchtext.data.Iterator(\n",
    "        val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "    test_dl = torchtext.data.Iterator(\n",
    "        test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "    return train_dl, val_dl, test_dl, TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "no vectors found at .vector_cache/data/wiki-news-300d-1M.vec",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f15eb5881dfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(\n\u001b[0;32m----> 2\u001b[0;31m     max_length=256, batch_size=24)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-f59fca2be6f7>\u001b[0m in \u001b[0;36mget_IMDb_DataLoaders_and_TEXT\u001b[0;34m(max_length, batch_size)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# torchtextで単語ベクトルとして英語学習済みモデルを読み込みます\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0menglish_fasttext_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/wiki-news-300d-1M.vec'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;31m# ベクトル化したバージョンのボキャブラリーを作成します\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, cache, url, unk_init, max_vectors)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munk_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0munk_init\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munk_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_vectors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchtext/vocab.py\u001b[0m in \u001b[0;36mcache\u001b[0;34m(self, name, cache, url, max_vectors)\u001b[0m\n\u001b[1;32m    370\u001b[0m                             \u001b[0mtar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'no vectors found at {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading vectors from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: no vectors found at .vector_cache/data/wiki-news-300d-1M.vec"
     ]
    }
   ],
   "source": [
    "train_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(\n",
    "    max_length=256, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "\n",
    "## ミニバッチ\n",
    "batch = next(iter(train_dl))\n",
    "\n",
    "## モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "\n",
    "## 入出力\n",
    "x = batch.Text[0]\n",
    "x1 = net1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 256])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-2eb11627bd44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=300, max_seq_len=256):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        \n",
    "        # 単語ベクトルの次元数\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 単語の順番posとベクトルの次元位置iの(p, i)によって一意に定まる表を作成する\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(\n",
    "                                        pos / (10000 ** ((2*i)/d_model)))\n",
    "                pe[pos, i+1] = math.cos(\n",
    "                                        pos / (10000 ** ((2*(i+1))/d_model)))\n",
    "        \n",
    "        self.pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # 勾配を計算しないようにする\n",
    "        self.pe.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 入力xとPositional Encoderを足し算する\n",
    "        ret = math.sqrt(self.d_model)*x + self.pe\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "\n",
    "## モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "\n",
    "## 入出力\n",
    "x = batch.Text[0]\n",
    "x1 = net1(x)\n",
    "x2 = net2(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力テンソルサイズ torch.Size([24, 256, 300])\n",
      "出力テンソルサイズ torch.Size([24, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "print('入力テンソルサイズ', x1.shape)\n",
    "print('出力テンソルサイズ', x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBlockモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNormalization: 特徴量の正規化を行う\n",
    "# Dropout: 過学習防止\n",
    "# Attention\n",
    "# FeedForward: 特徴量変換\n",
    "# からなる\n",
    "# * 実際のTransformerのAttentionではMulti-Headed Attentionを採用している\n",
    "# <pad>の部分にはmask=0をつけるが，Attentionでは-1e9とすることでsoftmaxの出力を0にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attentionの作成\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 特徴量の作成\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 出力の全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Attentionの大きさ調整の変数\n",
    "        self.d_k = d_model\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        \n",
    "        # Attentionの値を計算する\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # maskを計算\n",
    "        mask = mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        # softmaxで規格化する\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "        \n",
    "        # AttentionをValueと掛け算\n",
    "        output = torch.matmul(normalized_weights, v)\n",
    "        \n",
    "        # 特徴量を変換\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForwardの作成\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Blockの作成\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LayerNorm層\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Attention層\n",
    "        self.attn = Attention(d_model)\n",
    "        \n",
    "        # 全結合層\n",
    "        self.ff = FeedForward(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # 正規化とAttention\n",
    "        x_normalized = self.norm_1(x)\n",
    "        output, normalized_weights = self.attn(\n",
    "            x_normalized, x_normalized, x_normalized, mask)\n",
    "        \n",
    "        x2 = x + self.dropout_1(output)\n",
    "        \n",
    "        # 正規化と全結合層構築\n",
    "        x_normalized2 = self.norm_2(x2)\n",
    "        output = x2 + self.dropout_2(self.ff(x_normalized2))\n",
    "        \n",
    "        return output, normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "\n",
    "## モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "\n",
    "## maskの作成\n",
    "x = batch.Text[0]\n",
    "input_pad = 1   # padding ID\n",
    "input_mask = (x != input_pad)\n",
    "# print(input_mask[0])\n",
    "\n",
    "## 入出力\n",
    "x1 = net1(x)\n",
    "x2 = net2(x1)\n",
    "x3, normalized_weights = net3(x2, input_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classificationHeadモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_model=300, output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        # 重み初期化\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = x[:, 0, :]   # 各文の先頭の単語の特徴量を取り出す\n",
    "        out = self.linear(x0)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassification(nn.Module):\n",
    "    def __init__(self, text_embedding_vectors, d_model=300, max_seq_len=256,\n",
    "                           output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # モデルの構築\n",
    "        self.net1 = Embedder(text_embedding_vectors)\n",
    "        self.net2 = PositionalEncoder(d_model, max_seq_len)\n",
    "        self.net3_1 = TransformerBlock(d_model)\n",
    "        self.net3_2 = TransformerBlock(d_model)\n",
    "        self.net4 = ClassificationHead(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.net1(x)\n",
    "        x2 = self.net2(x1)\n",
    "        x3_1, normalized_weights_1 = self.net3_1(x2, mask)\n",
    "        x3_2, normalized_weights_2 = self.net3_2(x3_1, mask)\n",
    "        x4 = self.net4(x3_2)\n",
    "        return x4, normalized_weights_1, normalized_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "\n",
    "## モデル構築\n",
    "net = TransformerClassification(TEXT.vocab.vectors, d_model=300, max_seq_len=256,\n",
    "                               output_dim=2)\n",
    "\n",
    "## 入出力\n",
    "x = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (x != input_pad)\n",
    "out, normalized_weights_1, normalized_weights_2 = net(x, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの学習・推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "train_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(\n",
    "    max_length=256, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {'train': train_dl, 'val': val_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "net = TransformerClassification(\n",
    "    text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (norm_1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm_2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): Attention(\n",
       "    (q_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (v_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (k_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n",
       "  )\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメータの初期化を定義\n",
    "def weights_init(m):\n",
    "    classname =  m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "# 訓練モード\n",
    "net.train()\n",
    "\n",
    "# パラメータ初期化\n",
    "net.net3_1.apply(weights_init)\n",
    "net.net3_2.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と検証\n",
    "\n",
    "def train_model(net, datalloaders_dict, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('----start----')\n",
    "    net.to(device)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "            \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                inputs = batch.Text[0].to(device)\n",
    "                labels = batch.Label.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    # maskの作成\n",
    "                    input_pad = 1\n",
    "                    input_mask = (inputs != input_pad)\n",
    "                    \n",
    "                    # Transformerに入力\n",
    "                    outputs, _, _ = net(inputs, input_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # 更新\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # 結果の計算\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[pahse].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[pahse].dataset)\n",
    "            \n",
    "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                                                                         epoch+1,\n",
    "                                                                         num_epochs,\n",
    "                                                                         pahse,\n",
    "                                                                         epoch_loss,\n",
    "                                                                         epoch_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----start----\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected device cuda:0 but got device cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-a5e7d69af4eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnet_trained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-61-fce9ca7adf85>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, datalloaders_dict, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0;31m# Transformerに入力\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-e4c03fb67101>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mx3_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_weights_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet3_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mx3_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_weights_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet3_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-34-7d3ae3af5bfe>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# 入力xとPositional Encoderを足し算する\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected device cuda:0 but got device cpu"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "net_trained = train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
