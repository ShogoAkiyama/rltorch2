{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 単語分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "m_t = MeCab.Tagger('-Ochasen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '機械学習が好きです'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "機械\tキカイ\t機械\t名詞-一般\t\t\n",
      "学習\tガクシュウ\t学習\t名詞-サ変接続\t\t\n",
      "が\tガ\tが\t助詞-格助詞-一般\t\t\n",
      "好き\tスキ\t好き\t名詞-形容動詞語幹\t\t\n",
      "です\tデス\tです\t助動詞\t特殊・デス\t基本形\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "print(m_t.parse(text).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_mecab(text):\n",
    "    text = m_t.parse(text) # mecab\n",
    "    ret = text.strip().split()  # text処理\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing_text(text):\n",
    "    # 前処理\n",
    "    text = re.sub('\\r', '', text) # \n",
    "    text = re.sub('\\n', '', text) # 改行\n",
    "    text = re.sub(' ', '', text) # 半角\n",
    "    text = re.sub('　　', '', text) # 全角\n",
    "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenizerの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text) # textの前処理\n",
    "    ret = tokenizer_mecab(text) # mecabの単語分割\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "max_length = 25\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing,\n",
    "                                                 use_vocab=True, lower=True, include_lengths=True, \n",
    "                                                 batch_first=True, fix_length=max_length)\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasetの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='text_train.tsv',\n",
    "    validation='text_val.tsv', test='text_test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vars(train_ds[0])['Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ボキャブラリーの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'王と王子と女王と姫と男性と女性がいました。': 1,\n",
       "         '機械学習が好きです。': 1,\n",
       "         '本章から自然言語処理に取り組みます。': 1,\n",
       "         '本章では商品レビューの短い文章に対して、その文章がネガティブな評価をしている文章なのか、ポジティブな評価をしている文章なのか、2値のクラス分類する分類モデルを構築します。': 1})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(train_ds, min_freq=1)\n",
    "TEXT.vocab.freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fe22deeaa90>>, {'<unk>': 0, '<pad>': 1, '本章から自然言語処理に取り組みます。': 2, '本章では商品レビューの短い文章に対して、その文章がネガティブな評価をしている文章なのか、ポジティブな評価をしている文章なのか、2値のクラス分類する分類モデルを構築します。': 3, '機械学習が好きです。': 4, '王と王子と女王と姫と男性と女性がいました。': 5})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl= torchtext.data.Iterator(train_ds, batch_size=2, train=True)\n",
    "val_dl = torchtext.data.Iterator(val_ds, batch_size=2, train=False, sort=False)\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=2, train=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1],\n",
      "        [4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]]), tensor([1, 1]))\n",
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "batch = next(iter(train_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDbデータセットの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 訓練データの作成\n",
    "with open('./data/IMDb_train.tsv', 'w') as f:\n",
    "\n",
    "    path = './data/aclImdb/train/pos/'\n",
    "    for fname in glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding='utf-8') as ff:\n",
    "            text = ff.readline()\n",
    "            text = text.replace('\\t', ' ')\n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "\n",
    "    path = './data/aclImdb/train/neg/'\n",
    "    for fname in glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding='utf-8') as ff:\n",
    "            text = ff.readline()\n",
    "            text = text.replace('\\t', ' ')\n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータの作成\n",
    "with open('./data/IMDb_test.tsv', 'w') as f:\n",
    "\n",
    "    path = './data/aclImdb/test/pos/'\n",
    "    for fname in glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding='utf-8') as ff:\n",
    "            text = ff.readline()\n",
    "            text = text.replace('\\t', ' ')\n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    path = './data/aclImdb/test/neg/'\n",
    "    for fname in glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding='utf-8') as ff:\n",
    "            text = ff.readline()\n",
    "            text = text.replace('\\t', ' ')\n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "\n",
    "# 前処理\n",
    "def preprocessing_text(text):\n",
    "    text = re.sub('<br />', '', text)\n",
    "    \n",
    "    # カンマ・ピリオド以外の記号をスペースに変換\n",
    "    for p in string.punctuation:\n",
    "        if (p =='.') or (p == ','):\n",
    "            continue\n",
    "        else:\n",
    "            text = text.replace(p, ' ')\n",
    "        \n",
    "    # ピリオドの前後にはスペースを入れる\n",
    "    text = text.replace('.', ' . ')\n",
    "    text = text.replace(',', ' , ')\n",
    "    text = re.sub(r'[0-9 ０-９]', '0', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# 分かち書き\n",
    "def tokenizer_punctuation(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "# 前処理と分かち書きをまとめる\n",
    "def tokenizer_with_preprocessing(text):\n",
    "    text = preprocessing_text(text)\n",
    "    ret = tokenizer_punctuation(text)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I0lick0cats0.0']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_with_preprocessing('I lick cats.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasetの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n",
    "max_length = 256\n",
    "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                            lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "    path='./data/', train='IMDb_train.tsv',\n",
    "    test='IMDb_test.tsv', format='tsv',\n",
    "    fields=[('Text', TEXT), ('Label', LABEL)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text': ['very0different0topic0treated0in0this0film0.00a0straightforward0and0simple0description0of0local0chinese0customs0,00by0looking0at0the0daily0operation0of0a0public0bath0,00run0by0the0old0owner0and0his0retarded0son0,00when0older0son0returns0home0,00wrongly0believing0his0father0has0died0.00how0every0man0in0town0makes0his0daily0visit0to0chat0,00play0games0,00discuss0personal0matters0and0get0honest0advice0,00besides0the0usual0spa0like0therapies0.00when0old0man0dies0,00strong0and0loyal0family0ties0make0older0son0take0charge0,00so0public0bath0operation0is0not0disrupted0.00and0finally0,00the0arrival0of0modernization0to0end0this0way0of0spending0relaxed0hours0and0getting0along0.00the0public0bath0has0to0be0demolished0,00making0place0for0a0commercial0complex0to0be0constructed0.0'], 'Label': '1'}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train_val_ds[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練と検証を分ける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "train_ds, val_ds = train_val_ds.split(split_ratio=0.8, random_state=random.seed(1234))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ボキャブラリーの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_fasttext_vocabs = Vectors(name='./data/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "単語の次元数 300\n",
      "単語数 999994\n"
     ]
    }
   ],
   "source": [
    "print('単語の次元数', english_fasttext_vocabs.dim)\n",
    "print('単語数', len(english_fasttext_vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_ds, vectors=english_fasttext_vocabs, min_freq=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7f1e0932d5d0>>, {'<unk>': 0, '<pad>': 1, '<cls>': 2, '<eos>': 3})\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)\n",
    "print(TEXT.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoaderの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torchtext.data.Iterator(train_ds, batch_size=24, train=True)\n",
    "val_dl = torchtext.data.Iterator(val_ds, batch_size=24, train=False, sort=False)\n",
    "test_dl = torchtext.data.Iterator(test_ds, batch_size=24, train=False, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[2, 0, 3,  ..., 1, 1, 1],\n",
      "        [2, 0, 3,  ..., 1, 1, 1],\n",
      "        [2, 0, 3,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [2, 0, 3,  ..., 1, 1, 1],\n",
      "        [2, 0, 3,  ..., 1, 1, 1],\n",
      "        [2, 0, 3,  ..., 1, 1, 1]]), tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]))\n",
      "tensor([1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_dl))\n",
    "print(batch.Text)\n",
    "print(batch.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedder (単語数　→ 単語数*分散表現数)\n",
    "# Positional Encoder 単語数*分散表現数の位置情報を加える(単語数*分散表現数 → 単語数*分散表現数)\n",
    "# Transformer Blockモジュール"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Embedder(nn.Module):\n",
    "    def __init__(self, text_embedding_vectors):\n",
    "        super(Embedder, self).__init__()\n",
    "        \n",
    "        self.embeddings = nn.Embedding.from_pretrained(\n",
    "        embeddings=text_embedding_vectors, freeze=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.embeddings(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     11
    ]
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import io\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import math\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "\n",
    "def get_IMDb_DataLoaders_and_TEXT(max_length=256, batch_size=24):\n",
    "    \"\"\"IMDbのDataLoaderとTEXTオブジェクトを取得する。 \"\"\"\n",
    "\n",
    "    # 訓練データのtsvファイルを作成します\n",
    "    f = open('./data/IMDb_train.tsv', 'w')\n",
    "\n",
    "    path = './data/aclImdb/train/pos/'\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "\n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "\n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    path = './data/aclImdb/train/neg/'\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "\n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "\n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    f.close()\n",
    "\n",
    "   # テストデータの作成\n",
    "    f = open('./data/IMDb_test.tsv', 'w')\n",
    "\n",
    "    path = './data/aclImdb/test/pos/'\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "\n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "\n",
    "            text = text+'\\t'+'1'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "\n",
    "    path = './data/aclImdb/test/neg/'\n",
    "    for fname in glob.glob(os.path.join(path, '*.txt')):\n",
    "        with io.open(fname, 'r', encoding=\"utf-8\") as ff:\n",
    "            text = ff.readline()\n",
    "\n",
    "            # タブがあれば消しておきます\n",
    "            text = text.replace('\\t', \" \")\n",
    "\n",
    "            text = text+'\\t'+'0'+'\\t'+'\\n'\n",
    "            f.write(text)\n",
    "    f.close()\n",
    "\n",
    "    def preprocessing_text(text):\n",
    "        # 改行コードを消去\n",
    "        text = re.sub('<br />', '', text)\n",
    "\n",
    "        # カンマ、ピリオド以外の記号をスペースに置換\n",
    "        for p in string.punctuation:\n",
    "            if (p == \".\") or (p == \",\"):\n",
    "                continue\n",
    "            else:\n",
    "                text = text.replace(p, \" \")\n",
    "\n",
    "        # ピリオドなどの前後にはスペースを入れておく\n",
    "        text = text.replace(\".\", \" . \")\n",
    "        text = text.replace(\",\", \" , \")\n",
    "        return text\n",
    "\n",
    "    # 分かち書き（今回はデータが英語で、簡易的にスペースで区切る）\n",
    "    def tokenizer_punctuation(text):\n",
    "        return text.strip().split()\n",
    "\n",
    "\n",
    "    # 前処理と分かち書きをまとめた関数を定義\n",
    "    def tokenizer_with_preprocessing(text):\n",
    "        text = preprocessing_text(text)\n",
    "        ret = tokenizer_punctuation(text)\n",
    "        return ret\n",
    "\n",
    "\n",
    "    # データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
    "    # max_length\n",
    "    TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_with_preprocessing, use_vocab=True,\n",
    "                                lower=True, include_lengths=True, batch_first=True, fix_length=max_length, init_token=\"<cls>\", eos_token=\"<eos>\")\n",
    "    LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
    "\n",
    "    # フォルダ「data」から各tsvファイルを読み込みます\n",
    "    train_val_ds, test_ds = torchtext.data.TabularDataset.splits(\n",
    "        path='./data/', train='IMDb_train.tsv',\n",
    "        test='IMDb_test.tsv', format='tsv',\n",
    "        fields=[('Text', TEXT), ('Label', LABEL)])\n",
    "\n",
    "    # torchtext.data.Datasetのsplit関数で訓練データとvalidationデータを分ける\n",
    "    train_ds, val_ds = train_val_ds.split(\n",
    "        split_ratio=0.8, random_state=random.seed(1234))\n",
    "\n",
    "    # torchtextで単語ベクトルとして英語学習済みモデルを読み込みます\n",
    "    english_fasttext_vectors = Vectors(name='data/wiki-news-300d-1M.vec')\n",
    "\n",
    "    # ベクトル化したバージョンのボキャブラリーを作成します\n",
    "    TEXT.build_vocab(train_ds, vectors=english_fasttext_vectors, min_freq=10)\n",
    "\n",
    "    # DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
    "    train_dl = torchtext.data.Iterator(\n",
    "        train_ds, batch_size=batch_size, train=True)\n",
    "\n",
    "    val_dl = torchtext.data.Iterator(\n",
    "        val_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "    test_dl = torchtext.data.Iterator(\n",
    "        test_ds, batch_size=batch_size, train=False, sort=False)\n",
    "\n",
    "    return train_dl, val_dl, test_dl, TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(\n",
    "    max_length=256, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "\n",
    "## ミニバッチ\n",
    "batch = next(iter(train_dl))\n",
    "\n",
    "## モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "\n",
    "## 入出力\n",
    "x = batch.Text[0]\n",
    "x1 = net1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 256])\n",
      "torch.Size([24, 256, 300])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    def __init__(self, d_model=300, max_seq_len=256):\n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # 単語ベクトルの次元数\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 単語の順番posとベクトルの次元位置iの(p, i)によって一意に定まる表を作成する\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = math.sin(\n",
    "                                        pos / (10000 ** ((2*i)/d_model)))\n",
    "                pe[pos, i+1] = math.cos(\n",
    "                                        pos / (10000 ** ((2*(i+1))/d_model)))\n",
    "        \n",
    "        self.pe = pe.to(device).unsqueeze(0)\n",
    "        \n",
    "        # 勾配を計算しないようにする\n",
    "        self.pe.requires_grad = False\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 入力xとPositional Encoderを足し算する\n",
    "        ret = math.sqrt(self.d_model)*x + self.pe\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 動作確認\n",
    "\n",
    "# ## モデル構築\n",
    "# net1 = Embedder(TEXT.vocab.vectors)\n",
    "# net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "\n",
    "# ## 入出力\n",
    "# x = batch.Text[0]\n",
    "# x1 = net1(x)\n",
    "# x2 = net2(x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('入力テンソルサイズ', x1.shape)\n",
    "# print('出力テンソルサイズ', x2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TransformerBlockモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LayerNormalization: 特徴量の正規化を行う\n",
    "# Dropout: 過学習防止\n",
    "# Attention\n",
    "# FeedForward: 特徴量変換\n",
    "# からなる\n",
    "# * 実際のTransformerのAttentionではMulti-Headed Attentionを採用している\n",
    "# <pad>の部分にはmask=0をつけるが，Attentionでは-1e9とすることでsoftmaxの出力を0にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attentionの作成\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, d_model=300):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 特徴量の作成\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # 出力の全結合層\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Attentionの大きさ調整の変数\n",
    "        self.d_k = d_model\n",
    "        \n",
    "    def forward(self, q, k, v, mask):\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        \n",
    "        # Attentionの値を計算する\n",
    "        weights = torch.matmul(q, k.transpose(1, 2)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        # maskを計算\n",
    "        mask = mask.unsqueeze(1)\n",
    "        weights = weights.masked_fill(mask==0, -1e9)\n",
    "        \n",
    "        # softmaxで規格化する\n",
    "        normalized_weights = F.softmax(weights, dim=-1)\n",
    "        \n",
    "        # AttentionをValueと掛け算\n",
    "        output = torch.matmul(normalized_weights, v)\n",
    "        \n",
    "        # 特徴量を変換\n",
    "        output = self.out(output)\n",
    "        \n",
    "        return output, normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeedForwardの作成\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer Blockの作成\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # LayerNorm層\n",
    "        self.norm_1 = nn.LayerNorm(d_model)\n",
    "        self.norm_2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Attention層\n",
    "        self.attn = Attention(d_model)\n",
    "        \n",
    "        # 全結合層\n",
    "        self.ff = FeedForward(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # 正規化とAttention\n",
    "        x_normalized = self.norm_1(x)\n",
    "        output, normalized_weights = self.attn(\n",
    "            x_normalized, x_normalized, x_normalized, mask)\n",
    "        \n",
    "        x2 = x + self.dropout_1(output)\n",
    "        \n",
    "        # 正規化と全結合層構築\n",
    "        x_normalized2 = self.norm_2(x2)\n",
    "        output = x2 + self.dropout_2(self.ff(x_normalized2))\n",
    "        \n",
    "        return output, normalized_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 動作確認\n",
    "\n",
    "## モデル構築\n",
    "net1 = Embedder(TEXT.vocab.vectors)\n",
    "net2 = PositionalEncoder(d_model=300, max_seq_len=256)\n",
    "net3 = TransformerBlock(d_model=300)\n",
    "\n",
    "## maskの作成\n",
    "x = batch.Text[0]\n",
    "input_pad = 1   # padding ID\n",
    "input_mask = (x != input_pad)\n",
    "# print(input_mask[0])\n",
    "\n",
    "## 入出力\n",
    "x1 = net1(x)\n",
    "x2 = net2(x1)\n",
    "x3, normalized_weights = net3(x2, input_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# classificationHeadモジュール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, d_model=300, output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 全結合層\n",
    "        self.linear = nn.Linear(d_model, output_dim)\n",
    "        \n",
    "        # 重み初期化\n",
    "        nn.init.normal_(self.linear.weight, std=0.02)\n",
    "        nn.init.normal_(self.linear.bias, 0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x0 = x[:, 0, :]   # 各文の先頭の単語の特徴量を取り出す\n",
    "        out = self.linear(x0)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassification(nn.Module):\n",
    "    def __init__(self, text_embedding_vectors, d_model=300, max_seq_len=256,\n",
    "                           output_dim=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # モデルの構築\n",
    "        self.net1 = Embedder(text_embedding_vectors)\n",
    "        self.net2 = PositionalEncoder(d_model, max_seq_len)\n",
    "        self.net3_1 = TransformerBlock(d_model)\n",
    "        self.net3_2 = TransformerBlock(d_model)\n",
    "        self.net4 = ClassificationHead(d_model, output_dim)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x1 = self.net1(x)\n",
    "        x2 = self.net2(x1)\n",
    "        x3_1, normalized_weights_1 = self.net3_1(x2, mask)\n",
    "        x3_2, normalized_weights_2 = self.net3_2(x3_1, mask)\n",
    "        x4 = self.net4(x3_2)\n",
    "        return x4, normalized_weights_1, normalized_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TransformerClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c033264cf8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## モデル構築\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m net = TransformerClassification(TEXT.vocab.vectors, d_model=300, max_seq_len=256,\n\u001b[0m\u001b[1;32m      5\u001b[0m                                output_dim=2)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TransformerClassification' is not defined"
     ]
    }
   ],
   "source": [
    "# 動作確認\n",
    "\n",
    "## モデル構築\n",
    "net = TransformerClassification(TEXT.vocab.vectors, d_model=300, max_seq_len=256,\n",
    "                               output_dim=2)\n",
    "\n",
    "## 入出力\n",
    "x = batch.Text[0]\n",
    "input_pad = 1\n",
    "input_mask = (x != input_pad)\n",
    "out, normalized_weights_1, normalized_weights_2 = net(x, input_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformerの学習・推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 読み込み\n",
    "train_dl, val_dl, test_dl, TEXT = get_IMDb_DataLoaders_and_TEXT(\n",
    "    max_length=256, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {'train': train_dl, 'val': val_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの構築\n",
    "net = TransformerClassification(\n",
    "    text_embedding_vectors=TEXT.vocab.vectors, d_model=300, max_seq_len=256, output_dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerBlock(\n",
       "  (norm_1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm_2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): Attention(\n",
       "    (q_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (v_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (k_linear): Linear(in_features=300, out_features=300, bias=True)\n",
       "    (out): Linear(in_features=300, out_features=300, bias=True)\n",
       "  )\n",
       "  (ff): FeedForward(\n",
       "    (linear_1): Linear(in_features=300, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear_2): Linear(in_features=1024, out_features=300, bias=True)\n",
       "  )\n",
       "  (dropout_1): Dropout(p=0.1, inplace=False)\n",
       "  (dropout_2): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# パラメータの初期化を定義\n",
    "def weights_init(m):\n",
    "    classname =  m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        nn.init.kaiming_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "\n",
    "# 訓練モード\n",
    "net.train()\n",
    "\n",
    "# パラメータ初期化\n",
    "net.net3_1.apply(weights_init)\n",
    "net.net3_2.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 最適化手法\n",
    "learning_rate = 2e-5\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練と検証\n",
    "\n",
    "def train_model(net, datalloaders_dict, criterion, optimizer, num_epochs):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('----start----')\n",
    "    net.to(device)\n",
    "    \n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()\n",
    "            else:\n",
    "                net.eval()\n",
    "            \n",
    "            epoch_loss = 0.0\n",
    "            epoch_corrects = 0\n",
    "            \n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                inputs = batch.Text[0].to(device)\n",
    "                labels = batch.Label.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    \n",
    "                    # maskの作成\n",
    "                    input_pad = 1\n",
    "                    input_mask = (inputs != input_pad)\n",
    "                    \n",
    "                    # Transformerに入力\n",
    "                    outputs, _, _ = net(inputs, input_mask)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    \n",
    "                    # 更新\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                    # 結果の計算\n",
    "                    epoch_loss += loss.item() * inputs.size(0)\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "            \n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "            \n",
    "            print('Epoch {}/{} | {:^5} | Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                                                                         epoch+1,\n",
    "                                                                         num_epochs,\n",
    "                                                                         phase,\n",
    "                                                                         epoch_loss,\n",
    "                                                                         epoch_acc))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----start----\n",
      "Epoch 1/10 | train | Loss: 0.3472 Acc: 0.8495\n",
      "Epoch 1/10 |  val  | Loss: 0.3467 Acc: 0.8486\n",
      "Epoch 2/10 | train | Loss: 0.3416 Acc: 0.8508\n",
      "Epoch 2/10 |  val  | Loss: 0.3535 Acc: 0.8472\n",
      "Epoch 3/10 | train | Loss: 0.3307 Acc: 0.8589\n",
      "Epoch 3/10 |  val  | Loss: 0.3429 Acc: 0.8496\n",
      "Epoch 4/10 | train | Loss: 0.3268 Acc: 0.8605\n",
      "Epoch 4/10 |  val  | Loss: 0.3712 Acc: 0.8394\n",
      "Epoch 5/10 | train | Loss: 0.3184 Acc: 0.8655\n",
      "Epoch 5/10 |  val  | Loss: 0.4830 Acc: 0.7958\n",
      "Epoch 6/10 | train | Loss: 0.3168 Acc: 0.8647\n",
      "Epoch 6/10 |  val  | Loss: 0.3417 Acc: 0.8544\n",
      "Epoch 7/10 | train | Loss: 0.3077 Acc: 0.8674\n",
      "Epoch 7/10 |  val  | Loss: 0.3524 Acc: 0.8514\n",
      "Epoch 8/10 | train | Loss: 0.3042 Acc: 0.8703\n",
      "Epoch 8/10 |  val  | Loss: 0.3517 Acc: 0.8562\n",
      "Epoch 9/10 | train | Loss: 0.2962 Acc: 0.8749\n",
      "Epoch 9/10 |  val  | Loss: 0.3643 Acc: 0.8462\n",
      "Epoch 10/10 | train | Loss: 0.2911 Acc: 0.8767\n",
      "Epoch 10/10 |  val  | Loss: 0.3666 Acc: 0.8492\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "net_trained = train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# テストデータでの判定根拠の可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータでの正答率を求める\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "net_trained.eval()\n",
    "net_trained.to(device)\n",
    "\n",
    "epoch_corrects = 0\n",
    "\n",
    "for batch in test_dl:\n",
    "    inputs = batch.Text[0].to(device)\n",
    "    labels = batch.Label.to(device)\n",
    "    \n",
    "    with torch.set_grad_enabled(False):\n",
    "        input_pad = 1\n",
    "        input_mask = (inputs != input_pad)\n",
    "        \n",
    "        outputs, _, _ = net_trained(inputs, input_mask)\n",
    "        \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        epoch_corrects += torch.sum(preds == labels.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テストデータ25000個の正解率: 0.8509\n"
     ]
    }
   ],
   "source": [
    "epoch_acc = epoch_corrects.double() / len(test_dl.dataset)\n",
    "print('テストデータ{}個の正解率: {:.4f}'.format(len(test_dl.dataset), epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTMLを作成する関数を実装\n",
    "\n",
    "\n",
    "def highlight(word, attn):\n",
    "    \"Attentionの値が大きいと文字の背景が濃い赤になるhtmlを出力させる関数\"\n",
    "\n",
    "    html_color = '#%02X%02X%02X' % (\n",
    "        255, int(255*(1 - attn)), int(255*(1 - attn)))\n",
    "    return '<span style=\"background-color: {}\"> {}</span>'.format(html_color, word)\n",
    "\n",
    "\n",
    "def mk_html(index, batch, preds, normlized_weights_1, normlized_weights_2, TEXT):\n",
    "    \"HTMLデータを作成する\"\n",
    "\n",
    "    # indexの結果を抽出\n",
    "    sentence = batch.Text[0][index]  # 文章\n",
    "    label = batch.Label[index]  # ラベル\n",
    "    pred = preds[index]  # 予測\n",
    "\n",
    "    # indexのAttentionを抽出と規格化\n",
    "    attens1 = normlized_weights_1[index, 0, :]  # 0番目の<cls>のAttention\n",
    "    attens1 /= attens1.max()\n",
    "\n",
    "    attens2 = normlized_weights_2[index, 0, :]  # 0番目の<cls>のAttention\n",
    "    attens2 /= attens2.max()\n",
    "\n",
    "    # ラベルと予測結果を文字に置き換え\n",
    "    if label == 0:\n",
    "        label_str = \"Negative\"\n",
    "    else:\n",
    "        label_str = \"Positive\"\n",
    "\n",
    "    if pred == 0:\n",
    "        pred_str = \"Negative\"\n",
    "    else:\n",
    "        pred_str = \"Positive\"\n",
    "\n",
    "    # 表示用のHTMLを作成する\n",
    "    html = '正解ラベル：{}<br>推論ラベル：{}<br><br>'.format(label_str, pred_str)\n",
    "\n",
    "    # 1段目のAttention\n",
    "    html += '[TransformerBlockの1段目のAttentionを可視化]<br>'\n",
    "    for word, attn in zip(sentence, attens1):\n",
    "        html += highlight(TEXT.vocab.itos[word], attn)\n",
    "    html += \"<br><br>\"\n",
    "\n",
    "    # 2段目のAttention\n",
    "    html += '[TransformerBlockの2段目のAttentionを可視化]<br>'\n",
    "    for word, attn in zip(sentence, attens2):\n",
    "        html += highlight(TEXT.vocab.itos[word], attn)\n",
    "\n",
    "    html += \"<br><br>\"\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "正解ラベル：Positive<br>推論ラベル：Positive<br><br>[TransformerBlockの1段目のAttentionを可視化]<br><span style=\"background-color: #FFE9E9\"> <cls></span><span style=\"background-color: #FFF4F4\"> death</span><span style=\"background-color: #FFF6F6\"> wish</span><span style=\"background-color: #FFFEFE\"> 3</span><span style=\"background-color: #FFA6A6\"> brings</span><span style=\"background-color: #FFF8F8\"> back</span><span style=\"background-color: #FFFEFE\"> charles</span><span style=\"background-color: #FFFCFC\"> bronson</span><span style=\"background-color: #FFFDFD\"> as</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> vigilante</span><span style=\"background-color: #FFFEFE\"> paul</span><span style=\"background-color: #FFF9F9\"> <unk></span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> now</span><span style=\"background-color: #FFF5F5\"> retired</span><span style=\"background-color: #FFFBFB\"> .</span><span style=\"background-color: #FFE0E0\"> yeah</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> right</span><span style=\"background-color: #FFFCFC\"> .</span><span style=\"background-color: #FFFEFE\"> before</span><span style=\"background-color: #FFFDFD\"> long</span><span style=\"background-color: #FFECEC\"> <unk></span><span style=\"background-color: #FFF9F9\"> is</span><span style=\"background-color: #FFFAFA\"> back</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFCFC\"> his</span><span style=\"background-color: #FFFCFC\"> old</span><span style=\"background-color: #FFF5F5\"> ways</span><span style=\"background-color: #FFFBFB\"> .</span><span style=\"background-color: #FFFCFC\"> but</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFDFD\"> not</span><span style=\"background-color: #FFFEFE\"> just</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFDFD\"> few</span><span style=\"background-color: #FFF6F6\"> <unk></span><span style=\"background-color: #FFFDFD\"> at</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFFDFD\"> time</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFF8F8\"> gang</span><span style=\"background-color: #FFFDFD\"> who</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFF5F5\"> taken</span><span style=\"background-color: #FFFEFE\"> over</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFDFD\"> run</span><span style=\"background-color: #FFFDFD\"> down</span><span style=\"background-color: #FFF9F9\"> part</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFDFD\"> city</span><span style=\"background-color: #FFFCFC\"> new</span><span style=\"background-color: #FFFDFD\"> york</span><span style=\"background-color: #FFF5F5\"> again</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> by</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFF6F6\"> way</span><span style=\"background-color: #FFFBFB\"> .</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFEFE\"> war</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> so</span><span style=\"background-color: #FFF2F2\"> <unk></span><span style=\"background-color: #FFF3F3\"> <unk></span><span style=\"background-color: #FFFAFA\"> out</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFF3F3\"> big</span><span style=\"background-color: #FFF4F4\"> guns</span><span style=\"background-color: #FFF2F2\"> literally</span><span style=\"background-color: #FFFEFE\"> for</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFAFA\"> body</span><span style=\"background-color: #FFFCFC\"> count</span><span style=\"background-color: #FFF6F6\"> <unk></span><span style=\"background-color: #FFFCFC\"> his</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFAFA\"> rivals</span><span style=\"background-color: #FFFDFD\"> anything</span><span style=\"background-color: #FFF8F8\"> stallone</span><span style=\"background-color: #FFFEFE\"> or</span><span style=\"background-color: #FFF3F3\"> schwarzenegger</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFAFA\"> come</span><span style=\"background-color: #FFFCFC\"> up</span><span style=\"background-color: #FFFDFD\"> with</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFF5F5\"> movie</span><span style=\"background-color: #FFFAFA\"> is</span><span style=\"background-color: #FFFBFB\"> actually</span><span style=\"background-color: #FFCACA\"> somewhat</span><span style=\"background-color: #FFCBCB\"> fun</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFEAEA\"> watch</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF6F6\"> particularly</span><span style=\"background-color: #FFFEFE\"> for</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFFCFC\"> few</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFAFA\"> liners</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> course</span><span style=\"background-color: #FFB4B4\"> bad</span><span style=\"background-color: #FFF7F7\"> guys</span><span style=\"background-color: #FFE6E6\"> die</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> or</span><span style=\"background-color: #FFFDFD\"> get</span><span style=\"background-color: #FFE8E8\"> severely</span><span style=\"background-color: #FFFAFA\"> injured</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFF5F5\"> creative</span><span style=\"background-color: #FFF3F3\"> ways</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> which</span><span style=\"background-color: #FFF4F4\"> is</span><span style=\"background-color: #FFBDBD\"> always</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFC3C3\"> good</span><span style=\"background-color: #FFF0F0\"> thing</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFDDDD\"> violence</span><span style=\"background-color: #FFF8F8\"> is</span><span style=\"background-color: #FFFCFC\"> not</span><span style=\"background-color: #FFFDFD\"> as</span><span style=\"background-color: #FFFAFA\"> personal</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFBFB\"> this</span><span style=\"background-color: #FFFCFC\"> one</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF3F3\"> obviously</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFECEC\"> much</span><span style=\"background-color: #FFF7F7\"> grander</span><span style=\"background-color: #FFFAFA\"> scale</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFCFCF\"> sort</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFF9F9\"> like</span><span style=\"background-color: #FFFDFD\"> bombing</span><span style=\"background-color: #FFF7F7\"> your</span><span style=\"background-color: #FFFCFC\"> victims</span><span style=\"background-color: #FFFEFE\"> from</span><span style=\"background-color: #FFE1E1\"> afar</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF8F8\"> rather</span><span style=\"background-color: #FFFCFC\"> than</span><span style=\"background-color: #FFFBFB\"> one</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFBFB\"> one</span><span style=\"background-color: #FFF5F5\"> combat</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFF2F2\"> movie</span><span style=\"background-color: #FFF8F8\"> is</span><span style=\"background-color: #FFEAEA\"> lighter</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFF8F8\"> tone</span><span style=\"background-color: #FFFCFC\"> than</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFF5F5\"> 1st</span><span style=\"background-color: #FFFEFE\"> 2</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> making</span><span style=\"background-color: #FFFDFD\"> it</span><span style=\"background-color: #FFDADA\"> easier</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFDFDF\"> watch</span><span style=\"background-color: #FFF7F7\"> .</span><span style=\"background-color: #FFFCFC\"> there</span><span style=\"background-color: #FFF9F9\"> s</span><span style=\"background-color: #FFFDFD\"> not</span><span style=\"background-color: #FFF1F1\"> much</span><span style=\"background-color: #FFEAEA\"> import</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFF9F9\"> film</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFFDFD\"> more</span><span style=\"background-color: #FF7373\"> escapism</span><span style=\"background-color: #FFFDFD\"> than</span><span style=\"background-color: #FFFDFD\"> anything</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFF8F8\"> also</span><span style=\"background-color: #FF0000\"> cheesy</span><span style=\"background-color: #FFFDFD\"> at</span><span style=\"background-color: #FFFEFE\"> times</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFF6F6\"> pseudo</span><span style=\"background-color: #FFD6D6\"> inspirational</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> but</span><span style=\"background-color: #FFE4E4\"> hose</span><span style=\"background-color: #FFE8E8\"> scenes</span><span style=\"background-color: #FFFCFC\"> fall</span><span style=\"background-color: #FFF5F5\"> flat</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFCFC\"> they</span><span style=\"background-color: #FFFBFB\"> should</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFE0E0\"> left</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFEEEE\"> original</span><span style=\"background-color: #FFF6F6\"> death</span><span style=\"background-color: #FFF7F7\"> wish</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFFBFB\"> own</span><span style=\"background-color: #FFFDFD\"> without</span><span style=\"background-color: #FFD9D9\"> sequels</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> but</span><span style=\"background-color: #FFF9F9\"> since</span><span style=\"background-color: #FFFCFC\"> they</span><span style=\"background-color: #FFE3E3\"> didn</span><span style=\"background-color: #FFF2F2\"> t</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> they</span><span style=\"background-color: #FFFAFA\"> should</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFCFC\"> stopped</span><span style=\"background-color: #FFFBFB\"> here</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFEFE\"> 8</span><span style=\"background-color: #FFFEFE\"> 10</span><span style=\"background-color: #FFF3F3\"> <eos></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><br><br>[TransformerBlockの2段目のAttentionを可視化]<br><span style=\"background-color: #FFFEFE\"> <cls></span><span style=\"background-color: #FFFEFE\"> death</span><span style=\"background-color: #FFFEFE\"> wish</span><span style=\"background-color: #FFFEFE\"> 3</span><span style=\"background-color: #FFFEFE\"> brings</span><span style=\"background-color: #FFFEFE\"> back</span><span style=\"background-color: #FFFEFE\"> charles</span><span style=\"background-color: #FFFEFE\"> bronson</span><span style=\"background-color: #FFFEFE\"> as</span><span style=\"background-color: #FFFEFE\"> one</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> vigilante</span><span style=\"background-color: #FFFEFE\"> paul</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> now</span><span style=\"background-color: #FFFEFE\"> retired</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFAFA\"> yeah</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> right</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> before</span><span style=\"background-color: #FFFEFE\"> long</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> back</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFDFD\"> his</span><span style=\"background-color: #FFFEFE\"> old</span><span style=\"background-color: #FFFDFD\"> ways</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> but</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFAFA\"> not</span><span style=\"background-color: #FFFAFA\"> just</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFFEFE\"> few</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> at</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFCFC\"> a</span><span style=\"background-color: #FFFCFC\"> gang</span><span style=\"background-color: #FFFBFB\"> who</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFDFD\"> taken</span><span style=\"background-color: #FFFEFE\"> over</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> run</span><span style=\"background-color: #FFFEFE\"> down</span><span style=\"background-color: #FFFEFE\"> part</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> city</span><span style=\"background-color: #FFFEFE\"> new</span><span style=\"background-color: #FFFEFE\"> york</span><span style=\"background-color: #FFFEFE\"> again</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> by</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> way</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF9F9\"> this</span><span style=\"background-color: #FFFCFC\"> time</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFEFE\"> war</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> so</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> out</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> big</span><span style=\"background-color: #FFFEFE\"> guns</span><span style=\"background-color: #FFFEFE\"> literally</span><span style=\"background-color: #FFFEFE\"> for</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFECEC\"> body</span><span style=\"background-color: #FFFEFE\"> count</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFDFD\"> his</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFF1F1\"> rivals</span><span style=\"background-color: #FFF2F2\"> anything</span><span style=\"background-color: #FFFEFE\"> stallone</span><span style=\"background-color: #FFFEFE\"> or</span><span style=\"background-color: #FFFEFE\"> schwarzenegger</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFEFE\"> come</span><span style=\"background-color: #FFFEFE\"> up</span><span style=\"background-color: #FFFEFE\"> with</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFBFB\"> actually</span><span style=\"background-color: #FFFDFD\"> somewhat</span><span style=\"background-color: #FF0000\"> fun</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> watch</span><span style=\"background-color: #FFFDFD\"> ,</span><span style=\"background-color: #FFFEFE\"> particularly</span><span style=\"background-color: #FFFEFE\"> for</span><span style=\"background-color: #FFFAFA\"> a</span><span style=\"background-color: #FFFDFD\"> few</span><span style=\"background-color: #FFFAFA\"> one</span><span style=\"background-color: #FFFEFE\"> liners</span><span style=\"background-color: #FFFDFD\"> and</span><span style=\"background-color: #FFFBFB\"> of</span><span style=\"background-color: #FFFCFC\"> course</span><span style=\"background-color: #FFF0F0\"> bad</span><span style=\"background-color: #FFF9F9\"> guys</span><span style=\"background-color: #FFFEFE\"> die</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> or</span><span style=\"background-color: #FFFEFE\"> get</span><span style=\"background-color: #FFFCFC\"> severely</span><span style=\"background-color: #FFFEFE\"> injured</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFDFD\"> creative</span><span style=\"background-color: #FFFDFD\"> ways</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> which</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> always</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFECEC\"> good</span><span style=\"background-color: #FFFDFD\"> thing</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> violence</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFF4F4\"> not</span><span style=\"background-color: #FFFEFE\"> as</span><span style=\"background-color: #FFFEFE\"> personal</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFBFB\"> this</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF8F8\"> obviously</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFF2F2\"> much</span><span style=\"background-color: #FFFEFE\"> grander</span><span style=\"background-color: #FFFEFE\"> scale</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> sort</span><span style=\"background-color: #FFFCFC\"> of</span><span style=\"background-color: #FFF7F7\"> like</span><span style=\"background-color: #FFFEFE\"> bombing</span><span style=\"background-color: #FFFEFE\"> your</span><span style=\"background-color: #FFFEFE\"> victims</span><span style=\"background-color: #FFFEFE\"> from</span><span style=\"background-color: #FFFEFE\"> afar</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> rather</span><span style=\"background-color: #FFFEFE\"> than</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFEFE\"> combat</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> lighter</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFBFB\"> tone</span><span style=\"background-color: #FFFDFD\"> than</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> 1st</span><span style=\"background-color: #FFFEFE\"> 2</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> making</span><span style=\"background-color: #FFFAFA\"> it</span><span style=\"background-color: #FFF5F5\"> easier</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFEFE\"> watch</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> there</span><span style=\"background-color: #FFFEFE\"> s</span><span style=\"background-color: #FFF1F1\"> not</span><span style=\"background-color: #FFE9E9\"> much</span><span style=\"background-color: #FFFEFE\"> import</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFF8F8\"> this</span><span style=\"background-color: #FFFEFE\"> film</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFDFD\"> more</span><span style=\"background-color: #FFFEFE\"> escapism</span><span style=\"background-color: #FFFDFD\"> than</span><span style=\"background-color: #FFE7E7\"> anything</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFFEFE\"> also</span><span style=\"background-color: #FFF5F5\"> cheesy</span><span style=\"background-color: #FFFEFE\"> at</span><span style=\"background-color: #FFFDFD\"> times</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFE4E4\"> pseudo</span><span style=\"background-color: #FFFBFB\"> inspirational</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> but</span><span style=\"background-color: #FFFEFE\"> hose</span><span style=\"background-color: #FFFEFE\"> scenes</span><span style=\"background-color: #FFFEFE\"> fall</span><span style=\"background-color: #FFFEFE\"> flat</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> they</span><span style=\"background-color: #FFFEFE\"> should</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFEFE\"> left</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFFAFA\"> original</span><span style=\"background-color: #FFFEFE\"> death</span><span style=\"background-color: #FFFEFE\"> wish</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFBFB\"> its</span><span style=\"background-color: #FFF6F6\"> own</span><span style=\"background-color: #FFFCFC\"> without</span><span style=\"background-color: #FFFEFE\"> sequels</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> but</span><span style=\"background-color: #FFFDFD\"> since</span><span style=\"background-color: #FFFEFE\"> they</span><span style=\"background-color: #FFFDFD\"> didn</span><span style=\"background-color: #FFFEFE\"> t</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> they</span><span style=\"background-color: #FFFEFE\"> should</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFCFC\"> stopped</span><span style=\"background-color: #FFFAFA\"> here</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> 8</span><span style=\"background-color: #FFFEFE\"> 10</span><span style=\"background-color: #FFFEFE\"> <eos></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "batch = next(iter(test_dl))\n",
    "\n",
    "inputs = batch.Text[0].to(device)\n",
    "labels = batch.Label.to(device)\n",
    "\n",
    "input_pad = 1\n",
    "input_mask = (inputs != input_pad)\n",
    "\n",
    "outputs, normilized_weights_1, normilized_weights_2 = net_trained(inputs, input_mask)\n",
    "_, preds = torch.max(outputs, 1)\n",
    "\n",
    "index = 10\n",
    "html_output = mk_html(index, batch, preds, normilized_weights_1, normilized_weights_2, TEXT)\n",
    "HTML(html_output)  # HTML形式で出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "正解ラベル：Positive<br>推論ラベル：Positive<br><br>[TransformerBlockの1段目のAttentionを可視化]<br><span style=\"background-color: #FFE9E9\"> <cls></span><span style=\"background-color: #FFF4F4\"> death</span><span style=\"background-color: #FFF6F6\"> wish</span><span style=\"background-color: #FFFEFE\"> 3</span><span style=\"background-color: #FFA6A6\"> brings</span><span style=\"background-color: #FFF8F8\"> back</span><span style=\"background-color: #FFFEFE\"> charles</span><span style=\"background-color: #FFFCFC\"> bronson</span><span style=\"background-color: #FFFDFD\"> as</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> vigilante</span><span style=\"background-color: #FFFEFE\"> paul</span><span style=\"background-color: #FFF9F9\"> <unk></span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> now</span><span style=\"background-color: #FFF5F5\"> retired</span><span style=\"background-color: #FFFBFB\"> .</span><span style=\"background-color: #FFE0E0\"> yeah</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> right</span><span style=\"background-color: #FFFCFC\"> .</span><span style=\"background-color: #FFFEFE\"> before</span><span style=\"background-color: #FFFDFD\"> long</span><span style=\"background-color: #FFECEC\"> <unk></span><span style=\"background-color: #FFF9F9\"> is</span><span style=\"background-color: #FFFAFA\"> back</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFCFC\"> his</span><span style=\"background-color: #FFFCFC\"> old</span><span style=\"background-color: #FFF5F5\"> ways</span><span style=\"background-color: #FFFBFB\"> .</span><span style=\"background-color: #FFFCFC\"> but</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFDFD\"> not</span><span style=\"background-color: #FFFEFE\"> just</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFDFD\"> few</span><span style=\"background-color: #FFF6F6\"> <unk></span><span style=\"background-color: #FFFDFD\"> at</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFFDFD\"> time</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFF8F8\"> gang</span><span style=\"background-color: #FFFDFD\"> who</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFF5F5\"> taken</span><span style=\"background-color: #FFFEFE\"> over</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFDFD\"> run</span><span style=\"background-color: #FFFDFD\"> down</span><span style=\"background-color: #FFF9F9\"> part</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFDFD\"> city</span><span style=\"background-color: #FFFCFC\"> new</span><span style=\"background-color: #FFFDFD\"> york</span><span style=\"background-color: #FFF5F5\"> again</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> by</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFF6F6\"> way</span><span style=\"background-color: #FFFBFB\"> .</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFEFE\"> war</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> so</span><span style=\"background-color: #FFF2F2\"> <unk></span><span style=\"background-color: #FFF3F3\"> <unk></span><span style=\"background-color: #FFFAFA\"> out</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFF3F3\"> big</span><span style=\"background-color: #FFF4F4\"> guns</span><span style=\"background-color: #FFF2F2\"> literally</span><span style=\"background-color: #FFFEFE\"> for</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFAFA\"> body</span><span style=\"background-color: #FFFCFC\"> count</span><span style=\"background-color: #FFF6F6\"> <unk></span><span style=\"background-color: #FFFCFC\"> his</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFAFA\"> rivals</span><span style=\"background-color: #FFFDFD\"> anything</span><span style=\"background-color: #FFF8F8\"> stallone</span><span style=\"background-color: #FFFEFE\"> or</span><span style=\"background-color: #FFF3F3\"> schwarzenegger</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFAFA\"> come</span><span style=\"background-color: #FFFCFC\"> up</span><span style=\"background-color: #FFFDFD\"> with</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFF5F5\"> movie</span><span style=\"background-color: #FFFAFA\"> is</span><span style=\"background-color: #FFFBFB\"> actually</span><span style=\"background-color: #FFCACA\"> somewhat</span><span style=\"background-color: #FFCBCB\"> fun</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFEAEA\"> watch</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF6F6\"> particularly</span><span style=\"background-color: #FFFEFE\"> for</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFFCFC\"> few</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFAFA\"> liners</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFFEFE\"> course</span><span style=\"background-color: #FFB4B4\"> bad</span><span style=\"background-color: #FFF7F7\"> guys</span><span style=\"background-color: #FFE6E6\"> die</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> or</span><span style=\"background-color: #FFFDFD\"> get</span><span style=\"background-color: #FFE8E8\"> severely</span><span style=\"background-color: #FFFAFA\"> injured</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFF5F5\"> creative</span><span style=\"background-color: #FFF3F3\"> ways</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> which</span><span style=\"background-color: #FFF4F4\"> is</span><span style=\"background-color: #FFBDBD\"> always</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFC3C3\"> good</span><span style=\"background-color: #FFF0F0\"> thing</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFDDDD\"> violence</span><span style=\"background-color: #FFF8F8\"> is</span><span style=\"background-color: #FFFCFC\"> not</span><span style=\"background-color: #FFFDFD\"> as</span><span style=\"background-color: #FFFAFA\"> personal</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFBFB\"> this</span><span style=\"background-color: #FFFCFC\"> one</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF3F3\"> obviously</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFECEC\"> much</span><span style=\"background-color: #FFF7F7\"> grander</span><span style=\"background-color: #FFFAFA\"> scale</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFCFCF\"> sort</span><span style=\"background-color: #FFFEFE\"> of</span><span style=\"background-color: #FFF9F9\"> like</span><span style=\"background-color: #FFFDFD\"> bombing</span><span style=\"background-color: #FFF7F7\"> your</span><span style=\"background-color: #FFFCFC\"> victims</span><span style=\"background-color: #FFFEFE\"> from</span><span style=\"background-color: #FFE1E1\"> afar</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF8F8\"> rather</span><span style=\"background-color: #FFFCFC\"> than</span><span style=\"background-color: #FFFBFB\"> one</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFBFB\"> one</span><span style=\"background-color: #FFF5F5\"> combat</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFF2F2\"> movie</span><span style=\"background-color: #FFF8F8\"> is</span><span style=\"background-color: #FFEAEA\"> lighter</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFF8F8\"> tone</span><span style=\"background-color: #FFFCFC\"> than</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFF5F5\"> 1st</span><span style=\"background-color: #FFFEFE\"> 2</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> making</span><span style=\"background-color: #FFFDFD\"> it</span><span style=\"background-color: #FFDADA\"> easier</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFDFDF\"> watch</span><span style=\"background-color: #FFF7F7\"> .</span><span style=\"background-color: #FFFCFC\"> there</span><span style=\"background-color: #FFF9F9\"> s</span><span style=\"background-color: #FFFDFD\"> not</span><span style=\"background-color: #FFF1F1\"> much</span><span style=\"background-color: #FFEAEA\"> import</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFF9F9\"> film</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFFDFD\"> more</span><span style=\"background-color: #FF7373\"> escapism</span><span style=\"background-color: #FFFDFD\"> than</span><span style=\"background-color: #FFFDFD\"> anything</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFF8F8\"> also</span><span style=\"background-color: #FF0000\"> cheesy</span><span style=\"background-color: #FFFDFD\"> at</span><span style=\"background-color: #FFFEFE\"> times</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFF6F6\"> pseudo</span><span style=\"background-color: #FFD6D6\"> inspirational</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> but</span><span style=\"background-color: #FFE4E4\"> hose</span><span style=\"background-color: #FFE8E8\"> scenes</span><span style=\"background-color: #FFFCFC\"> fall</span><span style=\"background-color: #FFF5F5\"> flat</span><span style=\"background-color: #FFFAFA\"> .</span><span style=\"background-color: #FFFCFC\"> they</span><span style=\"background-color: #FFFBFB\"> should</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFE0E0\"> left</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFEEEE\"> original</span><span style=\"background-color: #FFF6F6\"> death</span><span style=\"background-color: #FFF7F7\"> wish</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFFBFB\"> own</span><span style=\"background-color: #FFFDFD\"> without</span><span style=\"background-color: #FFD9D9\"> sequels</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> but</span><span style=\"background-color: #FFF9F9\"> since</span><span style=\"background-color: #FFFCFC\"> they</span><span style=\"background-color: #FFE3E3\"> didn</span><span style=\"background-color: #FFF2F2\"> t</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> they</span><span style=\"background-color: #FFFAFA\"> should</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFCFC\"> stopped</span><span style=\"background-color: #FFFBFB\"> here</span><span style=\"background-color: #FFF9F9\"> .</span><span style=\"background-color: #FFFEFE\"> 8</span><span style=\"background-color: #FFFEFE\"> 10</span><span style=\"background-color: #FFF3F3\"> <eos></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><br><br>[TransformerBlockの2段目のAttentionを可視化]<br><span style=\"background-color: #FFFEFE\"> <cls></span><span style=\"background-color: #FFFEFE\"> death</span><span style=\"background-color: #FFFEFE\"> wish</span><span style=\"background-color: #FFFEFE\"> 3</span><span style=\"background-color: #FFFEFE\"> brings</span><span style=\"background-color: #FFFEFE\"> back</span><span style=\"background-color: #FFFEFE\"> charles</span><span style=\"background-color: #FFFEFE\"> bronson</span><span style=\"background-color: #FFFEFE\"> as</span><span style=\"background-color: #FFFEFE\"> one</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> vigilante</span><span style=\"background-color: #FFFEFE\"> paul</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> now</span><span style=\"background-color: #FFFEFE\"> retired</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFAFA\"> yeah</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFBFB\"> right</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> before</span><span style=\"background-color: #FFFEFE\"> long</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> back</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFDFD\"> his</span><span style=\"background-color: #FFFEFE\"> old</span><span style=\"background-color: #FFFDFD\"> ways</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> but</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFAFA\"> not</span><span style=\"background-color: #FFFAFA\"> just</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFFEFE\"> few</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> at</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFFEFE\"> time</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFCFC\"> a</span><span style=\"background-color: #FFFCFC\"> gang</span><span style=\"background-color: #FFFBFB\"> who</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFDFD\"> taken</span><span style=\"background-color: #FFFEFE\"> over</span><span style=\"background-color: #FFFEFE\"> a</span><span style=\"background-color: #FFFEFE\"> run</span><span style=\"background-color: #FFFEFE\"> down</span><span style=\"background-color: #FFFEFE\"> part</span><span style=\"background-color: #FFFDFD\"> of</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> city</span><span style=\"background-color: #FFFEFE\"> new</span><span style=\"background-color: #FFFEFE\"> york</span><span style=\"background-color: #FFFEFE\"> again</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> by</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> way</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFF9F9\"> this</span><span style=\"background-color: #FFFCFC\"> time</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFEFE\"> war</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> so</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFEFE\"> out</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> big</span><span style=\"background-color: #FFFEFE\"> guns</span><span style=\"background-color: #FFFEFE\"> literally</span><span style=\"background-color: #FFFEFE\"> for</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFECEC\"> body</span><span style=\"background-color: #FFFEFE\"> count</span><span style=\"background-color: #FFFEFE\"> <unk></span><span style=\"background-color: #FFFDFD\"> his</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFF1F1\"> rivals</span><span style=\"background-color: #FFF2F2\"> anything</span><span style=\"background-color: #FFFEFE\"> stallone</span><span style=\"background-color: #FFFEFE\"> or</span><span style=\"background-color: #FFFEFE\"> schwarzenegger</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFEFE\"> come</span><span style=\"background-color: #FFFEFE\"> up</span><span style=\"background-color: #FFFEFE\"> with</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFCFC\"> this</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFBFB\"> actually</span><span style=\"background-color: #FFFDFD\"> somewhat</span><span style=\"background-color: #FF0000\"> fun</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFFEFE\"> watch</span><span style=\"background-color: #FFFDFD\"> ,</span><span style=\"background-color: #FFFEFE\"> particularly</span><span style=\"background-color: #FFFEFE\"> for</span><span style=\"background-color: #FFFAFA\"> a</span><span style=\"background-color: #FFFDFD\"> few</span><span style=\"background-color: #FFFAFA\"> one</span><span style=\"background-color: #FFFEFE\"> liners</span><span style=\"background-color: #FFFDFD\"> and</span><span style=\"background-color: #FFFBFB\"> of</span><span style=\"background-color: #FFFCFC\"> course</span><span style=\"background-color: #FFF0F0\"> bad</span><span style=\"background-color: #FFF9F9\"> guys</span><span style=\"background-color: #FFFEFE\"> die</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> or</span><span style=\"background-color: #FFFEFE\"> get</span><span style=\"background-color: #FFFCFC\"> severely</span><span style=\"background-color: #FFFEFE\"> injured</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFDFD\"> creative</span><span style=\"background-color: #FFFDFD\"> ways</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> which</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> always</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFECEC\"> good</span><span style=\"background-color: #FFFDFD\"> thing</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> violence</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFF4F4\"> not</span><span style=\"background-color: #FFFEFE\"> as</span><span style=\"background-color: #FFFEFE\"> personal</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFBFB\"> this</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFF8F8\"> obviously</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFDFD\"> a</span><span style=\"background-color: #FFF2F2\"> much</span><span style=\"background-color: #FFFEFE\"> grander</span><span style=\"background-color: #FFFEFE\"> scale</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> sort</span><span style=\"background-color: #FFFCFC\"> of</span><span style=\"background-color: #FFF7F7\"> like</span><span style=\"background-color: #FFFEFE\"> bombing</span><span style=\"background-color: #FFFEFE\"> your</span><span style=\"background-color: #FFFEFE\"> victims</span><span style=\"background-color: #FFFEFE\"> from</span><span style=\"background-color: #FFFEFE\"> afar</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> rather</span><span style=\"background-color: #FFFEFE\"> than</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFDFD\"> one</span><span style=\"background-color: #FFFEFE\"> combat</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> this</span><span style=\"background-color: #FFFEFE\"> movie</span><span style=\"background-color: #FFFEFE\"> is</span><span style=\"background-color: #FFFEFE\"> lighter</span><span style=\"background-color: #FFFEFE\"> in</span><span style=\"background-color: #FFFBFB\"> tone</span><span style=\"background-color: #FFFDFD\"> than</span><span style=\"background-color: #FFFEFE\"> the</span><span style=\"background-color: #FFFEFE\"> 1st</span><span style=\"background-color: #FFFEFE\"> 2</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> making</span><span style=\"background-color: #FFFAFA\"> it</span><span style=\"background-color: #FFF5F5\"> easier</span><span style=\"background-color: #FFFEFE\"> to</span><span style=\"background-color: #FFFEFE\"> watch</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFDFD\"> there</span><span style=\"background-color: #FFFEFE\"> s</span><span style=\"background-color: #FFF1F1\"> not</span><span style=\"background-color: #FFE9E9\"> much</span><span style=\"background-color: #FFFEFE\"> import</span><span style=\"background-color: #FFFDFD\"> to</span><span style=\"background-color: #FFF8F8\"> this</span><span style=\"background-color: #FFFEFE\"> film</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> its</span><span style=\"background-color: #FFFDFD\"> more</span><span style=\"background-color: #FFFEFE\"> escapism</span><span style=\"background-color: #FFFDFD\"> than</span><span style=\"background-color: #FFE7E7\"> anything</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFCFC\"> its</span><span style=\"background-color: #FFFEFE\"> also</span><span style=\"background-color: #FFF5F5\"> cheesy</span><span style=\"background-color: #FFFEFE\"> at</span><span style=\"background-color: #FFFDFD\"> times</span><span style=\"background-color: #FFFEFE\"> and</span><span style=\"background-color: #FFE4E4\"> pseudo</span><span style=\"background-color: #FFFBFB\"> inspirational</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFDFD\"> but</span><span style=\"background-color: #FFFEFE\"> hose</span><span style=\"background-color: #FFFEFE\"> scenes</span><span style=\"background-color: #FFFEFE\"> fall</span><span style=\"background-color: #FFFEFE\"> flat</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> they</span><span style=\"background-color: #FFFEFE\"> should</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFEFE\"> left</span><span style=\"background-color: #FFFDFD\"> the</span><span style=\"background-color: #FFFAFA\"> original</span><span style=\"background-color: #FFFEFE\"> death</span><span style=\"background-color: #FFFEFE\"> wish</span><span style=\"background-color: #FFFEFE\"> on</span><span style=\"background-color: #FFFBFB\"> its</span><span style=\"background-color: #FFF6F6\"> own</span><span style=\"background-color: #FFFCFC\"> without</span><span style=\"background-color: #FFFEFE\"> sequels</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFCFC\"> but</span><span style=\"background-color: #FFFDFD\"> since</span><span style=\"background-color: #FFFEFE\"> they</span><span style=\"background-color: #FFFDFD\"> didn</span><span style=\"background-color: #FFFEFE\"> t</span><span style=\"background-color: #FFFEFE\"> ,</span><span style=\"background-color: #FFFEFE\"> they</span><span style=\"background-color: #FFFEFE\"> should</span><span style=\"background-color: #FFFEFE\"> have</span><span style=\"background-color: #FFFCFC\"> stopped</span><span style=\"background-color: #FFFAFA\"> here</span><span style=\"background-color: #FFFEFE\"> .</span><span style=\"background-color: #FFFEFE\"> 8</span><span style=\"background-color: #FFFEFE\"> 10</span><span style=\"background-color: #FFFEFE\"> <eos></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><span style=\"background-color: #FFFFFF\"> <pad></span><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(html_output)  # HTML形式で出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
